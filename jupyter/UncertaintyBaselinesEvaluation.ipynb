{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02704f6e-2a29-45ba-afb6-fb67aa367a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score, brier_score_loss, mean_squared_error\n",
    "from scipy.special import kl_div\n",
    "from functools import reduce\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from scipy.stats import entropy, norm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac9f7d3-6976-4ec4-930d-b5d13fd58508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63d1d9-08b7-4cac-ae6f-34e05e3c2ab0",
   "metadata": {},
   "source": [
    "# Baseline 1: Ensemble of Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47abbcaf-8d2f-4122-a3e6-44aece33042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_files = glob.glob('/users/lucelo/UQLRM/metadata_single_mlp_*.tsv')\n",
    "reward_dfs = []\n",
    "for file in reward_files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=0)[['id', 'RewardScore', 'Dataset', 'Preference']]\n",
    "    chosen_df = df[df['Preference'] == 'chosen'].rename(columns={'RewardScore': 'RewardChosen'})\n",
    "    rejected_df = df[df['Preference'] == 'rejected'].rename(columns={'RewardScore': 'RewardRejected'})\n",
    "    merged_df = chosen_df.merge(rejected_df, on=['id', 'Dataset'], how='inner')\n",
    "    merged_df['RewardDiff'] = merged_df['RewardChosen'] - merged_df['RewardRejected']\n",
    "    merged_df['PreferenceProb'] = sigmoid(merged_df['RewardDiff'])\n",
    "    reward_dfs.append(merged_df)\n",
    "\n",
    "for i, df in enumerate(reward_dfs):\n",
    "    # Add unique suffixes to the column names\n",
    "    df.columns = [f\"{col}_{i}\" if col != 'id' else col for col in df.columns]\n",
    "\n",
    "final_rewards =  pd.concat(reward_dfs, axis=1)\n",
    "N = len(reward_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ad714a-0fb7-445f-8a8a-8f9f1d0d8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16123/16123 [02:18<00:00, 116.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_rw_distinct_pairs(final_rewards, N):\n",
    "    pairs = [(i, j) for i in range(N) for j in range(N)]\n",
    "    def compute_rw_diff_pairs(x):\n",
    "        # np.random.shuffle(pairs)\n",
    "        # pairs = pairs[:N]\n",
    "        diffs = []\n",
    "        for pair in pairs:\n",
    "            i,j = pair\n",
    "            diffs.append(x[f'RewardChosen_{i}'] - x[f'RewardRejected_{j}'])\n",
    "        return np.var(diffs)\n",
    "        \n",
    "    return final_rewards.progress_apply(lambda x: compute_rw_diff_pairs(x), axis=1)\n",
    "\n",
    "final_rewards['RwDiffDistinctPairs'] = compute_rw_distinct_pairs(final_rewards, len(reward_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d20a9e-9998-4758-a724-fcf931bb76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_covariance(final_rewards):\n",
    "    covariances = []\n",
    "    for index, row in final_rewards.iterrows():\n",
    "        rejected = [row[f'RewardRejected_{n}'] for n in range(len(reward_dfs))]\n",
    "        chosen = [row[f'RewardChosen_{i}'] for i in range(len(reward_dfs))]\n",
    "        covariance = np.cov(rejected, chosen)[0][1]\n",
    "        covariances.append(covariance)\n",
    "    return covariances\n",
    "\n",
    "final_rewards['Covariance'] = compute_reward_covariance(final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d87be29-37d8-48be-8c66-f99117ca4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unc_stats(final_rewards):\n",
    "    # 1. Reward Statistics\n",
    "    final_rewards['RwAverage'] = final_rewards.filter(like=\"Reward\", axis=1).mean(axis=1)\n",
    "    final_rewards['RwVariance'] = final_rewards.filter(like=\"Reward\", axis=1).var(axis=1)\n",
    "    \n",
    "    # 2. \"Chosen\" Reward Statistics\n",
    "    final_rewards['RwChosenAverage'] = final_rewards.filter(like=\"RewardChosen\", axis=1).mean(axis=1)\n",
    "    final_rewards['RwChosenVariance'] = final_rewards.filter(like=\"RewardChosen\", axis=1).var(axis=1)\n",
    "    \n",
    "    # 3. \"Rejected\" Reward Statistics\n",
    "    final_rewards['RwRejectedAverage'] = final_rewards.filter(like=\"RewardRejected\", axis=1).mean(axis=1)\n",
    "    final_rewards['RwRejectedVariance'] = final_rewards.filter(like=\"RewardRejected\", axis=1).var(axis=1)\n",
    "    \n",
    "    # 4. Reward Diff (r_chosen - r_rejected) Statistics\n",
    "    final_rewards['RwDiffAverage'] = final_rewards.filter(like=\"RewardDiff\", axis=1).mean(axis=1)\n",
    "    final_rewards['RwDiffVariance'] = final_rewards.filter(like=\"RewardDiff\", axis=1).var(axis=1)\n",
    "    \n",
    "    # 5. Variance Sum = Var(r_chosen) + Var(r_rejected)\n",
    "    final_rewards['RwVarianceSum'] = final_rewards['RwChosenVariance'] + final_rewards['RwRejectedVariance']\n",
    "    \n",
    "    # 6. Var(r_chosen, r_rejected) = Var(r_chosen) - Var(r_rejected) - 2*Cov(r_chosen, r_rejected)\n",
    "    # This is an analytical version of Variance computed in #4\n",
    "    final_rewards['RwDiffAnalyticalVariance'] = final_rewards['RwChosenVariance'] + final_rewards['RwRejectedVariance'] - 2*final_rewards['Covariance']\n",
    "    \n",
    "    # 7. Var(p), p = sigmoid(r_chosen - r_rejected) (Preference Probability)\n",
    "    final_rewards['PrefProbVariance'] = final_rewards.filter(like=\"PreferenceProb\", axis=1).var(axis=1)\n",
    "    final_rewards['PrefProbAverage'] = final_rewards.filter(like=\"PreferenceProb\", axis=1).mean(axis=1)\n",
    "    \n",
    "    # 8. Max Variance = max(Var(r_chosen), Var(r_rejected))\n",
    "    final_rewards['RewardMaxVariance'] = final_rewards[['RwChosenVariance', 'RwRejectedVariance']].max(axis=1)\n",
    "\n",
    "    #9. Max Prediction Interval\n",
    "    final_rewards['MaxPrefInterval'] = final_rewards.filter(like=\"PreferenceProb\", axis=1).max(axis=1) - final_rewards.filter(like=\"PreferenceProb\", axis=1).min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2b7c92-16b8-4e47-97e0-90caa2eae82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_unc_stats(final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517f05ad-103a-476c-8ac8-247085fe1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RewardChosen_0</th>\n",
       "      <th>Dataset_0</th>\n",
       "      <th>Preference_x_0</th>\n",
       "      <th>RewardRejected_0</th>\n",
       "      <th>Preference_y_0</th>\n",
       "      <th>RewardDiff_0</th>\n",
       "      <th>PreferenceProb_0</th>\n",
       "      <th>id</th>\n",
       "      <th>RewardChosen_1</th>\n",
       "      <th>...</th>\n",
       "      <th>RwRejectedAverage</th>\n",
       "      <th>RwRejectedVariance</th>\n",
       "      <th>RwDiffAverage</th>\n",
       "      <th>RwDiffVariance</th>\n",
       "      <th>RwVarianceSum</th>\n",
       "      <th>RwDiffAnalyticalVariance</th>\n",
       "      <th>PrefProbVariance</th>\n",
       "      <th>PrefProbAverage</th>\n",
       "      <th>RewardMaxVariance</th>\n",
       "      <th>MaxPrefInterval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801</td>\n",
       "      <td>-1.021655</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.308317</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.286662</td>\n",
       "      <td>0.571179</td>\n",
       "      <td>1801</td>\n",
       "      <td>-1.198608</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.222914</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.259599</td>\n",
       "      <td>0.029189</td>\n",
       "      <td>0.042980</td>\n",
       "      <td>0.029189</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.564097</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.174546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87053</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.526894</td>\n",
       "      <td>rejected</td>\n",
       "      <td>1.939894</td>\n",
       "      <td>0.874340</td>\n",
       "      <td>87053</td>\n",
       "      <td>1.505481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579809</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>1.808733</td>\n",
       "      <td>0.019871</td>\n",
       "      <td>0.043450</td>\n",
       "      <td>0.019871</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.858371</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>0.064916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59149</td>\n",
       "      <td>-0.104880</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.680847</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.575967</td>\n",
       "      <td>0.640139</td>\n",
       "      <td>59149</td>\n",
       "      <td>0.134865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.488183</td>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.620937</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.042683</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.649819</td>\n",
       "      <td>0.021960</td>\n",
       "      <td>0.121001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080</td>\n",
       "      <td>1.230862</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-2.328661</td>\n",
       "      <td>rejected</td>\n",
       "      <td>3.559523</td>\n",
       "      <td>0.972335</td>\n",
       "      <td>20080</td>\n",
       "      <td>1.231286</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.121717</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>3.213534</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>0.034659</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.960816</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>0.032511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72323</td>\n",
       "      <td>0.448723</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.174440</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.650938</td>\n",
       "      <td>72323</td>\n",
       "      <td>0.188774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180517</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>0.469449</td>\n",
       "      <td>0.035856</td>\n",
       "      <td>0.053090</td>\n",
       "      <td>0.035856</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.614342</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.193187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  RewardChosen_0 Dataset_0 Preference_x_0  RewardRejected_0  \\\n",
       "0   1801       -1.021655     train         chosen         -1.308317   \n",
       "1  87053        1.413000     train         chosen         -0.526894   \n",
       "2  59149       -0.104880     train         chosen         -0.680847   \n",
       "3  20080        1.230862     train         chosen         -2.328661   \n",
       "4  72323        0.448723     train         chosen         -0.174440   \n",
       "\n",
       "  Preference_y_0  RewardDiff_0  PreferenceProb_0     id  RewardChosen_1  ...  \\\n",
       "0       rejected      0.286662          0.571179   1801       -1.198608  ...   \n",
       "1       rejected      1.939894          0.874340  87053        1.505481  ...   \n",
       "2       rejected      0.575967          0.640139  59149        0.134865  ...   \n",
       "3       rejected      3.559523          0.972335  20080        1.231286  ...   \n",
       "4       rejected      0.623163          0.650938  72323        0.188774  ...   \n",
       "\n",
       "  RwRejectedAverage RwRejectedVariance  RwDiffAverage RwDiffVariance  \\\n",
       "0         -1.222914           0.024675       0.259599       0.029189   \n",
       "1         -0.579809           0.024868       1.808733       0.019871   \n",
       "2         -0.488183           0.020723       0.620937       0.018558   \n",
       "3         -2.121717           0.018112       3.213534       0.030726   \n",
       "4         -0.180517           0.026328       0.469449       0.035856   \n",
       "\n",
       "   RwVarianceSum  RwDiffAnalyticalVariance  PrefProbVariance  PrefProbAverage  \\\n",
       "0       0.042980                  0.029189          0.001746         0.564097   \n",
       "1       0.043450                  0.019871          0.000291         0.858371   \n",
       "2       0.042683                  0.018558          0.000957         0.649819   \n",
       "3       0.034659                  0.030726          0.000047         0.960816   \n",
       "4       0.053090                  0.035856          0.002010         0.614342   \n",
       "\n",
       "  RewardMaxVariance MaxPrefInterval  \n",
       "0          0.024675        0.174546  \n",
       "1          0.024868        0.064916  \n",
       "2          0.021960        0.121001  \n",
       "3          0.018112        0.032511  \n",
       "4          0.026763        0.193187  \n",
       "\n",
       "[5 rows x 312 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rewards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e9c88b-2838-4296-8f0e-c26025074c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_rewards['PrefProbVariance'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6797987-8a03-4d57-8bb6-d2892e33fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoVElEQVR4nO3dfXSU9Z3//1dCkgkBJiF4MknaCNmtlRupWJAYxZuWkKipguVsNza1keVA14buYs4XJPsFhCgCkWIWilJ6CtRTEOTst6hAMVNYoUoImBWVm0O1C4tHdsIuMQw3SzIkn98f/jLbSYIEuIbMZ/J8nDMHr+t6z3V9Pu9EfHldc80VY4wxAgAAsEhsdw8AAADgahFgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWievuAYRLa2urTp48qX79+ikmJqa7hwMAALrAGKOzZ88qMzNTsbGXP88StQHm5MmTysrK6u5hAACAa/DZZ5/p61//+mW3R22A6devn6QvG+B2ux3bbyAQUHV1tfLz8xUfH+/Yfnsq+uks+uks+uks+umsaO2n3+9XVlZW8L/jlxO1AabtspHb7XY8wCQlJcntdkfVL0x3oZ/Oop/Oop/Oop/OivZ+XunjH3yIFwAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6cd09gGg1aNbWDuuOLyrshpEAABB9OAMDAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANa56gCze/duPfLII8rMzFRMTIw2b94cst0Yo7lz5yojI0O9e/dWXl6ePvnkk5CahoYGFRcXy+12KyUlRZMnT9a5c+dCaj766CPde++9SkxMVFZWliorK69+dgAAICpddYA5f/68br/9dq1YsaLT7ZWVlVq2bJlWrlyp2tpa9enTRwUFBbp48WKwpri4WIcOHZLX69WWLVu0e/duTZ06Nbjd7/crPz9fAwcOVF1dnV588UXNmzdPq1atuoYpAgCAaHPVD3N86KGH9NBDD3W6zRijqqoqzZ49W+PHj5ckvfrqq/J4PNq8ebOKiop05MgRbd++Xfv379eoUaMkScuXL9fDDz+sJUuWKDMzU+vWrVNzc7NWr16thIQEDRs2TAcOHNDSpUtDgg4AAOiZHH0a9bFjx+Tz+ZSXlxdcl5ycrJycHNXU1KioqEg1NTVKSUkJhhdJysvLU2xsrGpra/XYY4+ppqZG9913nxISEoI1BQUFWrx4sb744gv179+/w7GbmprU1NQUXPb7/ZKkQCCgQCDg2Bzb9nWlfbp6mcu+F/+rq/1E19BPZ9FPZ9FPZ0VrP7s6H0cDjM/nkyR5PJ6Q9R6PJ7jN5/MpLS0tdBBxcUpNTQ2pyc7O7rCPtm2dBZiFCxdq/vz5HdZXV1crKSnpGmd0eV6v9yu3V47uuG7btm2OjyNaXKmfuDr001n001n001nR1s8LFy50qc7RANOdysvLVVZWFlz2+/3KyspSfn6+3G63Y8cJBALyer0aN26c4uPjL1t327y3O6w7OK/AsXFEi672E11DP51FP51FP50Vrf1su4JyJY4GmPT0dElSfX29MjIyguvr6+s1YsSIYM2pU6dC3nfp0iU1NDQE35+enq76+vqQmrbltpr2XC6XXC5Xh/Xx8fFh+cFeab9NLTGdvgedC9fPqaein86in86in86Ktn52dS6Ofg9Mdna20tPTtWPHjuA6v9+v2tpa5ebmSpJyc3PV2Niourq6YM3OnTvV2tqqnJycYM3u3btDroN5vV7deuutnV4+AgAAPctVB5hz587pwIEDOnDggKQvP7h74MABnThxQjExMZo+fbqef/55vfnmm/r444/14x//WJmZmZowYYIkaciQIXrwwQc1ZcoU7du3T++9956mTZumoqIiZWZmSpJ++MMfKiEhQZMnT9ahQ4e0ceNG/fM//3PIJSIAANBzXfUlpPfff1/f+c53gsttoaKkpERr167VzJkzdf78eU2dOlWNjY0aM2aMtm/frsTExOB71q1bp2nTpmns2LGKjY3VxIkTtWzZsuD25ORkVVdXq7S0VCNHjtRNN92kuXPncgs1AACQdA0B5oEHHpAxHW8RbhMTE6OKigpVVFRctiY1NVXr16//yuN861vf0h//+MerHR4AAOgBeBYSAACwDgEGAABYhwADAACsEzVfZNfdBs3a2t1DAACgxyDARLjOgtHxRYXdMBIAACIHl5AAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKzDbdQ3UPtborkdGgCAa0OA6UZ8xwsAANeGS0gAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIeHOUaYzh7wCAAAQnEGBgAAWIcAAwAArMMlpCjQ2WWn44sKu2EkAADcGAQYC/E5GQBAT8clJAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsE5cdw8AN8agWVs7rDu+qLAbRgIAwPVz/AxMS0uL5syZo+zsbPXu3Vt//dd/reeee07GmGCNMUZz585VRkaGevfurby8PH3yySch+2loaFBxcbHcbrdSUlI0efJknTt3zunhAgAACzkeYBYvXqxXXnlFv/jFL3TkyBEtXrxYlZWVWr58ebCmsrJSy5Yt08qVK1VbW6s+ffqooKBAFy9eDNYUFxfr0KFD8nq92rJli3bv3q2pU6c6PVwAAGAhxy8h7dmzR+PHj1dh4ZeXJwYNGqTXXntN+/btk/Tl2ZeqqirNnj1b48ePlyS9+uqr8ng82rx5s4qKinTkyBFt375d+/fv16hRoyRJy5cv18MPP6wlS5YoMzPT6WEDAACLOB5g7r77bq1atUp/+tOf9M1vflMffvih3n33XS1dulSSdOzYMfl8PuXl5QXfk5ycrJycHNXU1KioqEg1NTVKSUkJhhdJysvLU2xsrGpra/XYY491OG5TU5OampqCy36/X5IUCAQUCAQcm1/bvtrv09XLdFbebboyPif7cq0u109cG/rpLPrpLPrprGjtZ1fn43iAmTVrlvx+vwYPHqxevXqppaVFCxYsUHFxsSTJ5/NJkjweT8j7PB5PcJvP51NaWlroQOPilJqaGqxpb+HChZo/f36H9dXV1UpKSrruebXn9XpDlitHO36I67Jt27aQ5c7G176mO7XvJ64P/XQW/XQW/XRWtPXzwoULXapzPMC8/vrrWrdundavX69hw4bpwIEDmj59ujIzM1VSUuL04YLKy8tVVlYWXPb7/crKylJ+fr7cbrdjxwkEAvJ6vRo3bpzi4+OD62+b97Zjx3DCwXkFIcudja99TXe4XD9xbeins+ins+ins6K1n21XUK7E8QAzY8YMzZo1S0VFRZKk4cOH6z/+4z+0cOFClZSUKD09XZJUX1+vjIyM4Pvq6+s1YsQISVJ6erpOnToVst9Lly6poaEh+P72XC6XXC5Xh/Xx8fFh+cG2329TS4zjx7ge7efc2fgi6Rc+XD+nnop+Oot+Oot+Oiva+tnVuTh+F9KFCxcUGxu62169eqm1tVWSlJ2drfT0dO3YsSO43e/3q7a2Vrm5uZKk3NxcNTY2qq6uLlizc+dOtba2Kicnx+khAwAAyzh+BuaRRx7RggULdPPNN2vYsGH64IMPtHTpUv3d3/2dJCkmJkbTp0/X888/r1tuuUXZ2dmaM2eOMjMzNWHCBEnSkCFD9OCDD2rKlClauXKlAoGApk2bpqKiIu5AAgAAzgeY5cuXa86cOfrpT3+qU6dOKTMzUz/5yU80d+7cYM3MmTN1/vx5TZ06VY2NjRozZoy2b9+uxMTEYM26des0bdo0jR07VrGxsZo4caKWLVvm9HABAICFHA8w/fr1U1VVlaqqqi5bExMTo4qKClVUVFy2JjU1VevXr3d6eAAAIArwMEcAAGAdAgwAALAOT6OOUp09fRoAgGjBGRgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDp8kV0P1tmX3R1fVNgNIwEA4OpwBgYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1onr7gEgsgyatTVk+fiiwm4aCQAAl8cZGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOmEJMJ9//rl+9KMfacCAAerdu7eGDx+u999/P7jdGKO5c+cqIyNDvXv3Vl5enj755JOQfTQ0NKi4uFhut1spKSmaPHmyzp07F47hAgAAyzgeYL744gvdc889io+P1+9//3sdPnxYP//5z9W/f/9gTWVlpZYtW6aVK1eqtrZWffr0UUFBgS5evBisKS4u1qFDh+T1erVlyxbt3r1bU6dOdXq4AADAQo4/SmDx4sXKysrSmjVrguuys7OD/2yMUVVVlWbPnq3x48dLkl599VV5PB5t3rxZRUVFOnLkiLZv3679+/dr1KhRkqTly5fr4Ycf1pIlS5SZmen0sAEAgEUcDzBvvvmmCgoK9Dd/8zfatWuXvva1r+mnP/2ppkyZIkk6duyYfD6f8vLygu9JTk5WTk6OampqVFRUpJqaGqWkpATDiyTl5eUpNjZWtbW1euyxxzoct6mpSU1NTcFlv98vSQoEAgoEAo7Nr21f7ffp6mUcO0YkcbJ3X7X/cB+np6CfzqKfzqKfzorWfnZ1Po4HmH//93/XK6+8orKyMv3TP/2T9u/fr3/4h39QQkKCSkpK5PP5JEkejyfkfR6PJ7jN5/MpLS0tdKBxcUpNTQ3WtLdw4ULNnz+/w/rq6molJSU5MbUQXq83ZLlytOOHiAjbtm27Icdp309cH/rpLPrpLPrprGjr54ULF7pU53iAaW1t1ahRo/TCCy9Iku644w4dPHhQK1euVElJidOHCyovL1dZWVlw2e/3KysrS/n5+XK73Y4dJxAIyOv1aty4cYqPjw+uv23e244dI5IcnFcQ1v1frp+4NvTTWfTTWfTTWdHaz7YrKFfieIDJyMjQ0KFDQ9YNGTJE//Iv/yJJSk9PlyTV19crIyMjWFNfX68RI0YEa06dOhWyj0uXLqmhoSH4/vZcLpdcLleH9fHx8WH5wbbfb1NLjOPHiAS3zKnusO74okLHjxOun1NPRT+dRT+dRT+dFW397OpcHL8L6Z577tHRo0dD1v3pT3/SwIEDJX35gd709HTt2LEjuN3v96u2tla5ubmSpNzcXDU2Nqquri5Ys3PnTrW2tionJ8fpIQMAAMs4fgbm6aef1t13360XXnhBP/jBD7Rv3z6tWrVKq1atkiTFxMRo+vTpev7553XLLbcoOztbc+bMUWZmpiZMmCDpyzM2Dz74oKZMmaKVK1cqEAho2rRpKioq4g4kAADgfIC588479bvf/U7l5eWqqKhQdna2qqqqVFxcHKyZOXOmzp8/r6lTp6qxsVFjxozR9u3blZiYGKxZt26dpk2bprFjxyo2NlYTJ07UsmXLnB4uAACwkOMBRpK+973v6Xvf+95lt8fExKiiokIVFRWXrUlNTdX69evDMTwAAGA5noUEAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdeK6ewCwz6BZW0OWjy8q7KaRAAB6Ks7AAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYh9uocd3a31YtcWs1ACC8OAMDAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1wh5gFi1apJiYGE2fPj247uLFiyotLdWAAQPUt29fTZw4UfX19SHvO3HihAoLC5WUlKS0tDTNmDFDly5dCvdwAQCABcIaYPbv369f/vKX+ta3vhWy/umnn9Zbb72lTZs2adeuXTp58qS+//3vB7e3tLSosLBQzc3N2rNnj37zm99o7dq1mjt3bjiHCwAALBG2AHPu3DkVFxfrV7/6lfr37x9cf+bMGf3617/W0qVL9d3vflcjR47UmjVrtGfPHu3du1eSVF1drcOHD+u3v/2tRowYoYceekjPPfecVqxYoebm5nANGQAAWCJsAaa0tFSFhYXKy8sLWV9XV6dAIBCyfvDgwbr55ptVU1MjSaqpqdHw4cPl8XiCNQUFBfL7/Tp06FC4hgwAACwRF46dbtiwQf/2b/+m/fv3d9jm8/mUkJCglJSUkPUej0c+ny9Y85fhpW1727bONDU1qampKbjs9/slSYFAQIFA4Jrn0l7bvtrv09XLOHaMaNDVnl+un7g29NNZ9NNZ9NNZ0drPrs7H8QDz2Wef6R//8R/l9XqVmJjo9O4va+HChZo/f36H9dXV1UpKSnL8eF6vN2S5crTjh7Datm3brqq+fT9xfeins+ins+ins6KtnxcuXOhSneMBpq6uTqdOndK3v/3t4LqWlhbt3r1bv/jFL/T222+rublZjY2NIWdh6uvrlZ6eLklKT0/Xvn37QvbbdpdSW0175eXlKisrCy77/X5lZWUpPz9fbrfbqekpEAjI6/Vq3Lhxio+PD66/bd7bjh0jGhycV9Clusv1E9eGfjqLfjqLfjorWvvZdgXlShwPMGPHjtXHH38csm7SpEkaPHiwnnnmGWVlZSk+Pl47duzQxIkTJUlHjx7ViRMnlJubK0nKzc3VggULdOrUKaWlpUn6MmG63W4NHTq00+O6XC65XK4O6+Pj48Pyg22/36aWGMePYbOr7Xm4fk49Ff10Fv10Fv10VrT1s6tzcTzA9OvXT7fddlvIuj59+mjAgAHB9ZMnT1ZZWZlSU1Pldrv1s5/9TLm5ubrrrrskSfn5+Ro6dKieeOIJVVZWyufzafbs2SotLe00pAAAgJ4lLB/ivZKXXnpJsbGxmjhxopqamlRQUKCXX345uL1Xr17asmWLnnrqKeXm5qpPnz4qKSlRRUVFdwwXAABEmBsSYN55552Q5cTERK1YsUIrVqy47HsGDhx41R8EBQAAPUO3nIFB9Bs0a2vI8vFFhd00EgBANOJhjgAAwDqcgcEN0f6MjMRZGQDAteMMDAAAsA4BBgAAWIdLSNfotnlv8+V1AAB0E87AAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAg24zaNZW3TbvbUkK/gkAQFcQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWCeuuwcAtBk0a2uHdccXFXbDSAAAkY4zMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6/A9MIho7b8bhu+FAQBInIEBAAAWIsAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsI7jAWbhwoW688471a9fP6WlpWnChAk6evRoSM3FixdVWlqqAQMGqG/fvpo4caLq6+tDak6cOKHCwkIlJSUpLS1NM2bM0KVLl5weLiwzaNbWDi8AQM/jeIDZtWuXSktLtXfvXnm9XgUCAeXn5+v8+fPBmqefflpvvfWWNm3apF27dunkyZP6/ve/H9ze0tKiwsJCNTc3a8+ePfrNb36jtWvXau7cuU4PFwAAWCjO6R1u3749ZHnt2rVKS0tTXV2d7rvvPp05c0a//vWvtX79en33u9+VJK1Zs0ZDhgzR3r17ddddd6m6ulqHDx/WH/7wB3k8Ho0YMULPPfecnnnmGc2bN08JCQlODxsAAFjE8QDT3pkzZyRJqampkqS6ujoFAgHl5eUFawYPHqybb75ZNTU1uuuuu1RTU6Phw4fL4/EEawoKCvTUU0/p0KFDuuOOOzocp6mpSU1NTcFlv98vSQoEAgoEAo7Np21frljj2D57srY+Xk8/nfz52q6tF/TEGfTTWfTTWdHaz67OJ6wBprW1VdOnT9c999yj2267TZLk8/mUkJCglJSUkFqPxyOfzxes+cvw0ra9bVtnFi5cqPnz53dYX11draSkpOudSgfPjWp1fJ892fX0c9u2bQ6OJDp4vd7uHkJUoZ/Oop/OirZ+XrhwoUt1YQ0wpaWlOnjwoN59991wHkaSVF5errKysuCy3+9XVlaW8vPz5Xa7HTtOIBCQ1+vVnPdj1dQa49h+eypXrNFzo1qvq58H5xU4PCp7tf1+jhs3TvHx8d09HOvRT2fRT2dFaz/brqBcSdgCzLRp07Rlyxbt3r1bX//614Pr09PT1dzcrMbGxpCzMPX19UpPTw/W7Nu3L2R/bXcptdW053K55HK5OqyPj48Pyw+2qTVGTS0EGKdcTz+j6V9cp4Tr976nop/Oop/OirZ+dnUujt+FZIzRtGnT9Lvf/U47d+5UdnZ2yPaRI0cqPj5eO3bsCK47evSoTpw4odzcXElSbm6uPv74Y506dSpY4/V65Xa7NXToUKeHDAAALOP4GZjS0lKtX79eb7zxhvr16xf8zEpycrJ69+6t5ORkTZ48WWVlZUpNTZXb7dbPfvYz5ebm6q677pIk5efna+jQoXriiSdUWVkpn8+n2bNnq7S0tNOzLAAAoGdxPMC88sorkqQHHnggZP2aNWv05JNPSpJeeuklxcbGauLEiWpqalJBQYFefvnlYG2vXr20ZcsWPfXUU8rNzVWfPn1UUlKiiooKp4cLAAAs5HiAMebKt8MmJiZqxYoVWrFixWVrBg4cyN0lAACgUzwLCQAAWIcAAwAArEOAAQAA1iHAAAAA64T9WUjAjTZo1tYO644vKuyGkQAAwoUAA+t1FlgAANGNS0gAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDp8Ey96hPbf1sujBQDAbpyBAQAA1iHAAAAA63AJCT0ST6wGALtxBgYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHW4Cwm4DO5UAoDIxRkYAABgHQIMAACwDpeQgP9fZ5eMAACRiTMwAADAOgQYAABgHS4hAVeh/WUm7koCgO5BgAGuA7daA0D34BISAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6fA8MEGZdecYS3x0DAFeHMzAAAMA6nIEBHObUU615bAEAXB4BBogAToUeAOgpCDCAJXjuEgD8Lz4DAwAArEOAAQAA1iHAAAAA6xBgAACAdfgQL2Cx9h/sdfUyqhwt3TbvbTW1xEjig74AohMBBuhhnLqbibuiAHQnAgyADggnACIdn4EBAADW4QwMEOX4ll8A0YgAA6BLIYcgBCCSEGAAhA2fpQEQLgQYAI6JhrM0fzmHttvSAUSeiA4wK1as0Isvviifz6fbb79dy5cv1+jR/G0C2Kx9yOnsjMy1BCHO7AA9S8QGmI0bN6qsrEwrV65UTk6OqqqqVFBQoKNHjyotLa27hwfAIU6dtenusz9dCWYAnBOxAWbp0qWaMmWKJk2aJElauXKltm7dqtWrV2vWrFndPDoA+Go38vM/XQ1vhCpEk4gMMM3Nzaqrq1N5eXlwXWxsrPLy8lRTU9Ppe5qamtTU1BRcPnPmjCSpoaFBgUDAsbEFAgFduHBBcYFYtbTGOLbfniqu1ejChVb66RD66ay2fo74v/9PTVfoZ1f+Mv3G/3n9qsdQWz72ijVxl853aV+nT5++Yk3Owh1XPP611rT9/Xn69GnFx8d3ZchX3O/ljh8NrtRnJ/oZic6ePStJMsZ8daGJQJ9//rmRZPbs2ROyfsaMGWb06NGdvufZZ581knjx4sWLFy9eUfD67LPPvjIrROQZmGtRXl6usrKy4HJra6saGho0YMAAxcQ493+ifr9fWVlZ+uyzz+R2ux3bb09FP51FP51FP51FP50Vrf00xujs2bPKzMz8yrqIDDA33XSTevXqpfr6+pD19fX1Sk9P7/Q9LpdLLpcrZF1KSkq4hii32x1VvzDdjX46i346i346i346Kxr7mZycfMWaiHwWUkJCgkaOHKkdO/73+l9ra6t27Nih3NzcbhwZAACIBBF5BkaSysrKVFJSolGjRmn06NGqqqrS+fPng3clAQCAnitiA8zf/u3f6r/+6780d+5c+Xw+jRgxQtu3b5fH4+nWcblcLj377LMdLlfh2tBPZ9FPZ9FPZ9FPZ/X0fsYYc6X7lAAAACJLRH4GBgAA4KsQYAAAgHUIMAAAwDoEGAAAYJ2oDzArVqzQoEGDlJiYqJycHO3bt+8r6zdt2qTBgwcrMTFRw4cP17Zt20K2G2M0d+5cZWRkqHfv3srLy9Mnn3wSUtPQ0KDi4mK53W6lpKRo8uTJOnfuXEjNRx99pHvvvVeJiYnKyspSZWWlMxMOs0js58WLF/Xkk09q+PDhiouL04QJExybb7hFYj/feecdjR8/XhkZGerTp49GjBihdevWOTfpMIrEfh49elTf+c535PF4lJiYqL/6q7/S7NmzHX1GW7hEYj//0qeffqp+/fqF9UtLnRSJ/Tx+/LhiYmI6vPbu3evcxMPlep9bFMk2bNhgEhISzOrVq82hQ4fMlClTTEpKiqmvr++0/r333jO9evUylZWV5vDhw2b27NkmPj7efPzxx8GaRYsWmeTkZLN582bz4YcfmkcffdRkZ2eb//mf/wnWPPjgg+b22283e/fuNX/84x/NN77xDfP4448Ht585c8Z4PB5TXFxsDh48aF577TXTu3dv88tf/jJ8zXBApPbz3Llz5u///u/NqlWrTEFBgRk/fnzYeuCkSO3nggULzOzZs817771nPv30U1NVVWViY2PNW2+9Fb5mOCBS+/nnP//ZrF692hw4cMAcP37cvPHGGyYtLc2Ul5eHrxkOiNR+tmlubjajRo0yDz30kElOTnZ8/k6L1H4eO3bMSDJ/+MMfzH/+538GX83NzeFrhkOiOsCMHj3alJaWBpdbWlpMZmamWbhwYaf1P/jBD0xhYWHIupycHPOTn/zEGGNMa2urSU9PNy+++GJwe2Njo3G5XOa1114zxhhz+PBhI8ns378/WPP73//exMTEmM8//9wYY8zLL79s+vfvb5qamoI1zzzzjLn11luvc8bhFan9/EslJSXWBBgb+tnm4YcfNpMmTbr6Sd5ANvXz6aefNmPGjLn6Sd5Akd7PmTNnmh/96EdmzZo1VgSYSO1nW4D54IMPHJnnjRS1l5Cam5tVV1envLy84LrY2Fjl5eWppqam0/fU1NSE1EtSQUFBsP7YsWPy+XwhNcnJycrJyQnW1NTUKCUlRaNGjQrW5OXlKTY2VrW1tcGa++67TwkJCSHHOXr0qL744ovrnHl4RHI/bWRbP8+cOaPU1NSrn+gNYlM/P/30U23fvl3333//tU32Boj0fu7cuVObNm3SihUrrn+yN0Ck91OSHn30UaWlpWnMmDF68803r2/CN0jUBpj//u//VktLS4dv7vV4PPL5fJ2+x+fzfWV9259XqklLSwvZHhcXp9TU1JCazvbxl8eINJHcTxvZ1M/XX39d+/fvj+jHeNjQz7vvvluJiYm65ZZbdO+996qiouIqZ3njRHI/T58+rSeffFJr16615gGGkdzPvn376uc//7k2bdqkrVu3asyYMZowYYIVISZiHyUAoPv967/+qyZNmqRf/epXGjZsWHcPx2obN27U2bNn9eGHH2rGjBlasmSJZs6c2d3Dss6UKVP0wx/+UPfdd193DyUq3HTTTSorKwsu33nnnTp58qRefPFFPfroo904siuL2jMwN910k3r16qX6+vqQ9fX19UpPT+/0Penp6V9Z3/bnlWpOnToVsv3SpUtqaGgIqelsH395jEgTyf20kQ393LVrlx555BG99NJL+vGPf3yVM7yxbOhnVlaWhg4dqscff1yLFi3SvHnz1NLScpUzvTEiuZ87d+7UkiVLFBcXp7i4OE2ePFlnzpxRXFycVq9efY0zDq9I7mdncnJy9Omnn3ZhZt0ragNMQkKCRo4cqR07dgTXtba2aseOHcrNze30Pbm5uSH1kuT1eoP12dnZSk9PD6nx+/2qra0N1uTm5qqxsVF1dXXBmp07d6q1tVU5OTnBmt27d4fcRun1enXrrbeqf//+1znz8Ijkftoo0vv5zjvvqLCwUIsXL9bUqVOvf8JhFun9bK+1tVWBQECtra1XP9kbIJL7WVNTowMHDgRfFRUV6tevnw4cOKDHHnvMmQY4LJL72ZkDBw4oIyPj6id6o3X3p4jDacOGDcblcpm1a9eaw4cPm6lTp5qUlBTj8/mMMcY88cQTZtasWcH69957z8TFxZklS5aYI0eOmGeffbbT29ZSUlLMG2+8YT766CMzfvz4Tm9bu+OOO0xtba159913zS233BJy21pjY6PxeDzmiSeeMAcPHjQbNmwwSUlJVtxGHYn9NMaYQ4cOmQ8++MA88sgj5oEHHjAffPBBxH+qPlL7uXPnTpOUlGTKy8tDbqs8ffr0DejKtYvUfv72t781GzduNIcPHzZ//vOfzcaNG01mZqYpLi6+AV25dpHaz/ZsuQspUvu5du1as379enPkyBFz5MgRs2DBAhMbG2tWr159A7pyfaI6wBhjzPLly83NN99sEhISzOjRo83evXuD2+6//35TUlISUv/666+bb37zmyYhIcEMGzbMbN26NWR7a2urmTNnjvF4PMblcpmxY8eao0ePhtScPn3aPP7446Zv377G7XabSZMmmbNnz4bUfPjhh2bMmDHG5XKZr33ta2bRokXOTjxMIrWfAwcONJI6vCJdJPazpKSk017ef//9js/faZHYzw0bNphvf/vbpm/fvqZPnz5m6NCh5oUXXgj5j0ykisR+tmdLgDEmMvu5du1aM2TIEJOUlGTcbrcZPXq02bRpk/OTD4MYY4zprrM/AAAA1yJqPwMDAACiFwEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANb5/wCKD8WiqsGzjgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized_probs = final_rewards['PrefProbVariance'] / final_rewards['PrefProbVariance'].sum()\n",
    "normalized_probs.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c47fd671-c22c-4e3a-a4b7-7ca3bd0d9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, temperature=0.5):\n",
    "    e_x = np.exp(x / temperature)\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13150bc-cfd9-4bf5-9d35-812d82c022da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soft_probs = softmax(final_rewards['PrefProbVariance'], temperature=0.005)\n",
    "soft_probs.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf640704-6496-41b6-b1e7-e75f35f5d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce(final_rewards):\n",
    "    final_rewards['Error'] = (final_rewards['PrefProbAverage'] < 0.5) * 1.0\n",
    "    final_rewards['GT'] = 1.0\n",
    "    final_rewards['CrossEntropy'] = kl_div(final_rewards['GT'], final_rewards['PrefProbAverage'])\n",
    "    final_rewards['BrierScore'] = (final_rewards['GT'] - final_rewards['PrefProbAverage'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39103ccb-b96e-472d-aa6a-004e6f4c77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ce(final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f20035b6-cc5d-4259-9d18-4439f88cea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RewardChosen_0</th>\n",
       "      <th>Dataset_0</th>\n",
       "      <th>Preference_x_0</th>\n",
       "      <th>RewardRejected_0</th>\n",
       "      <th>Preference_y_0</th>\n",
       "      <th>RewardDiff_0</th>\n",
       "      <th>PreferenceProb_0</th>\n",
       "      <th>id</th>\n",
       "      <th>RewardChosen_1</th>\n",
       "      <th>...</th>\n",
       "      <th>RwVarianceSum</th>\n",
       "      <th>RwDiffAnalyticalVariance</th>\n",
       "      <th>PrefProbVariance</th>\n",
       "      <th>PrefProbAverage</th>\n",
       "      <th>RewardMaxVariance</th>\n",
       "      <th>MaxPrefInterval</th>\n",
       "      <th>Error</th>\n",
       "      <th>GT</th>\n",
       "      <th>CrossEntropy</th>\n",
       "      <th>BrierScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801</td>\n",
       "      <td>-1.021655</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.308317</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.286662</td>\n",
       "      <td>0.571179</td>\n",
       "      <td>1801</td>\n",
       "      <td>-1.198608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042980</td>\n",
       "      <td>0.029189</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.564097</td>\n",
       "      <td>0.024675</td>\n",
       "      <td>0.174546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.136626</td>\n",
       "      <td>0.190012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87053</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.526894</td>\n",
       "      <td>rejected</td>\n",
       "      <td>1.939894</td>\n",
       "      <td>0.874340</td>\n",
       "      <td>87053</td>\n",
       "      <td>1.505481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043450</td>\n",
       "      <td>0.019871</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.858371</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>0.064916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.020059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59149</td>\n",
       "      <td>-0.104880</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.680847</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.575967</td>\n",
       "      <td>0.640139</td>\n",
       "      <td>59149</td>\n",
       "      <td>0.134865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042683</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.649819</td>\n",
       "      <td>0.021960</td>\n",
       "      <td>0.121001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>0.122626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080</td>\n",
       "      <td>1.230862</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-2.328661</td>\n",
       "      <td>rejected</td>\n",
       "      <td>3.559523</td>\n",
       "      <td>0.972335</td>\n",
       "      <td>20080</td>\n",
       "      <td>1.231286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034659</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.960816</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72323</td>\n",
       "      <td>0.448723</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.174440</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.650938</td>\n",
       "      <td>72323</td>\n",
       "      <td>0.188774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053090</td>\n",
       "      <td>0.035856</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.614342</td>\n",
       "      <td>0.026763</td>\n",
       "      <td>0.193187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.101546</td>\n",
       "      <td>0.148732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 316 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  RewardChosen_0 Dataset_0 Preference_x_0  RewardRejected_0  \\\n",
       "0   1801       -1.021655     train         chosen         -1.308317   \n",
       "1  87053        1.413000     train         chosen         -0.526894   \n",
       "2  59149       -0.104880     train         chosen         -0.680847   \n",
       "3  20080        1.230862     train         chosen         -2.328661   \n",
       "4  72323        0.448723     train         chosen         -0.174440   \n",
       "\n",
       "  Preference_y_0  RewardDiff_0  PreferenceProb_0     id  RewardChosen_1  ...  \\\n",
       "0       rejected      0.286662          0.571179   1801       -1.198608  ...   \n",
       "1       rejected      1.939894          0.874340  87053        1.505481  ...   \n",
       "2       rejected      0.575967          0.640139  59149        0.134865  ...   \n",
       "3       rejected      3.559523          0.972335  20080        1.231286  ...   \n",
       "4       rejected      0.623163          0.650938  72323        0.188774  ...   \n",
       "\n",
       "  RwVarianceSum RwDiffAnalyticalVariance  PrefProbVariance PrefProbAverage  \\\n",
       "0      0.042980                 0.029189          0.001746        0.564097   \n",
       "1      0.043450                 0.019871          0.000291        0.858371   \n",
       "2      0.042683                 0.018558          0.000957        0.649819   \n",
       "3      0.034659                 0.030726          0.000047        0.960816   \n",
       "4      0.053090                 0.035856          0.002010        0.614342   \n",
       "\n",
       "   RewardMaxVariance  MaxPrefInterval  Error   GT CrossEntropy BrierScore  \n",
       "0           0.024675         0.174546    0.0  1.0     0.136626   0.190012  \n",
       "1           0.024868         0.064916    0.0  1.0     0.011090   0.020059  \n",
       "2           0.021960         0.121001    0.0  1.0     0.080880   0.122626  \n",
       "3           0.018112         0.032511    0.0  1.0     0.000788   0.001535  \n",
       "4           0.026763         0.193187    0.0  1.0     0.101546   0.148732  \n",
       "\n",
       "[5 rows x 316 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rewards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02dc7fc-9838-40bd-9b29-0d9da2e28f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(final_rewards, dataset_column):\n",
    "    train_df = final_rewards[final_rewards[dataset_column] == 'train']\n",
    "    test_df = final_rewards[final_rewards[dataset_column] == 'test']\n",
    "    eval_df = final_rewards[final_rewards[dataset_column] == 'eval']\n",
    "    ood_df = final_rewards[final_rewards[dataset_column] == 'ood']\n",
    "    return train_df, test_df, eval_df, ood_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbecc76f-4780-4b1b-a503-0c410199c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, eval_df, ood_df = split_dataset(final_rewards, \"Dataset_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f7921-f990-4bba-b850-dd76cf4044b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a0147c7-fa96-4293-bedc-6ad4340b4322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6882672519297213 1.7828431959087783\n",
      "-2.311974043781693 1.3708552275155041\n"
     ]
    }
   ],
   "source": [
    "def compute_quantiles(train_df):\n",
    "    chosen_p5 = train_df['RwChosenAverage'].quantile(0.05)\n",
    "    chosen_p95 = train_df['RwChosenAverage'].quantile(0.95)\n",
    "    \n",
    "    rej_p5 = train_df['RwRejectedAverage'].quantile(0.05)\n",
    "    rej_p95 = train_df['RwRejectedAverage'].quantile(0.95)\n",
    "    \n",
    "    print(chosen_p5, chosen_p95)\n",
    "    print(rej_p5, rej_p95)\n",
    "    return chosen_p5, chosen_p95, rej_p5, rej_p95\n",
    "\n",
    "def compute_outlier_unc(final_rewards, chosen_p5, chosen_p95, rej_p5, rej_p95):\n",
    "    # Heuristic: Get p5 and p95 in reward distributions (aka GDA)\n",
    "    final_rewards['TooHighOrTooLow'] = ((final_rewards['RwChosenAverage'] > chosen_p95) | (final_rewards['RwChosenAverage'] < chosen_p5) | (final_rewards['RwRejectedAverage'] < rej_p5) | (final_rewards['RwRejectedAverage'] > rej_p95)) * 1.0\n",
    "\n",
    "chosen_p5, chosen_p95, rej_p5, rej_p95 = compute_quantiles(train_df)\n",
    "compute_outlier_unc(final_rewards, chosen_p5, chosen_p95, rej_p5, rej_p95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cce5818-9d82-412d-92f7-51c29a2f3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gaussian_prob(df, pred_column, mean, std):\n",
    "    # Fit a Gaussian density with these parameters\n",
    "    gaussian_density = norm(loc=mean, scale=std)\n",
    "\n",
    "    # Return the probability associated with a new point s in this Gaussian\n",
    "    df[f'GDA_{pred_column}_Prob'] = df[pred_column].apply(gaussian_density.pdf)\n",
    "    df[f'GDA_{pred_column}_alpha'] = df[f'GDA_{pred_column}_Prob'].apply(lambda x: x if x < 0.5 else 1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "863f9ac9-5552-46ce-a662-09141952b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_chosen = train_df['RwChosenAverage'].mean()\n",
    "std_chosen = train_df['RwChosenAverage'].std()\n",
    "\n",
    "mean_rejected = train_df['RwRejectedAverage'].mean()\n",
    "std_rejected = train_df['RwRejectedAverage'].std()\n",
    "\n",
    "compute_gaussian_prob(final_rewards, \"RwChosenAverage\", mean_chosen, std_chosen)\n",
    "compute_gaussian_prob(final_rewards, \"RwRejectedAverage\", mean_rejected, std_rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "610ce576-11a9-4f68-9e9d-7066f04bb4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>RewardChosen_0</th>\n",
       "      <th>Dataset_0</th>\n",
       "      <th>Preference_x_0</th>\n",
       "      <th>RewardRejected_0</th>\n",
       "      <th>Preference_y_0</th>\n",
       "      <th>RewardDiff_0</th>\n",
       "      <th>PreferenceProb_0</th>\n",
       "      <th>id</th>\n",
       "      <th>RewardChosen_1</th>\n",
       "      <th>...</th>\n",
       "      <th>MaxPrefInterval</th>\n",
       "      <th>Error</th>\n",
       "      <th>GT</th>\n",
       "      <th>CrossEntropy</th>\n",
       "      <th>BrierScore</th>\n",
       "      <th>TooHighOrTooLow</th>\n",
       "      <th>GDA_RwChosenAverage_Prob</th>\n",
       "      <th>GDA_RwChosenAverage_alpha</th>\n",
       "      <th>GDA_RwRejectedAverage_Prob</th>\n",
       "      <th>GDA_RwRejectedAverage_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801</td>\n",
       "      <td>-1.021655</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.308317</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.286662</td>\n",
       "      <td>0.571179</td>\n",
       "      <td>1801</td>\n",
       "      <td>-1.198608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.136626</td>\n",
       "      <td>0.190012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217420</td>\n",
       "      <td>0.217420</td>\n",
       "      <td>0.281609</td>\n",
       "      <td>0.281609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87053</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.526894</td>\n",
       "      <td>rejected</td>\n",
       "      <td>1.939894</td>\n",
       "      <td>0.874340</td>\n",
       "      <td>87053</td>\n",
       "      <td>1.505481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.020059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.223827</td>\n",
       "      <td>0.223827</td>\n",
       "      <td>0.351744</td>\n",
       "      <td>0.351744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59149</td>\n",
       "      <td>-0.104880</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.680847</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.575967</td>\n",
       "      <td>0.640139</td>\n",
       "      <td>59149</td>\n",
       "      <td>0.134865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>0.122626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372784</td>\n",
       "      <td>0.372784</td>\n",
       "      <td>0.353587</td>\n",
       "      <td>0.353587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080</td>\n",
       "      <td>1.230862</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-2.328661</td>\n",
       "      <td>rejected</td>\n",
       "      <td>3.559523</td>\n",
       "      <td>0.972335</td>\n",
       "      <td>20080</td>\n",
       "      <td>1.231286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252674</td>\n",
       "      <td>0.252674</td>\n",
       "      <td>0.119711</td>\n",
       "      <td>0.119711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72323</td>\n",
       "      <td>0.448723</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.174440</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.650938</td>\n",
       "      <td>72323</td>\n",
       "      <td>0.188774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.101546</td>\n",
       "      <td>0.148732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369601</td>\n",
       "      <td>0.369601</td>\n",
       "      <td>0.342887</td>\n",
       "      <td>0.342887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>80580</td>\n",
       "      <td>1.554397</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>1.008231</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.546165</td>\n",
       "      <td>0.633245</td>\n",
       "      <td>80580</td>\n",
       "      <td>1.649850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.070629</td>\n",
       "      <td>0.109103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146877</td>\n",
       "      <td>0.146877</td>\n",
       "      <td>0.170765</td>\n",
       "      <td>0.170765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>52285</td>\n",
       "      <td>0.859001</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.549616</td>\n",
       "      <td>rejected</td>\n",
       "      <td>2.408617</td>\n",
       "      <td>0.917482</td>\n",
       "      <td>52285</td>\n",
       "      <td>0.817228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289912</td>\n",
       "      <td>0.289912</td>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.236659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>50293</td>\n",
       "      <td>1.536220</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.667574</td>\n",
       "      <td>rejected</td>\n",
       "      <td>3.203794</td>\n",
       "      <td>0.960977</td>\n",
       "      <td>50293</td>\n",
       "      <td>1.728942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141012</td>\n",
       "      <td>0.141012</td>\n",
       "      <td>0.231813</td>\n",
       "      <td>0.231813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>32555</td>\n",
       "      <td>-1.131761</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.406382</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.274621</td>\n",
       "      <td>0.568227</td>\n",
       "      <td>32555</td>\n",
       "      <td>-1.001117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108164</td>\n",
       "      <td>0.156795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189364</td>\n",
       "      <td>0.189364</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>70805</td>\n",
       "      <td>0.145257</td>\n",
       "      <td>train</td>\n",
       "      <td>chosen</td>\n",
       "      <td>0.965101</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.819845</td>\n",
       "      <td>0.305797</td>\n",
       "      <td>70805</td>\n",
       "      <td>-0.155879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144874</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.537587</td>\n",
       "      <td>0.509695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372210</td>\n",
       "      <td>0.372210</td>\n",
       "      <td>0.151532</td>\n",
       "      <td>0.151532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  RewardChosen_0 Dataset_0 Preference_x_0  RewardRejected_0  \\\n",
       "0    1801       -1.021655     train         chosen         -1.308317   \n",
       "1   87053        1.413000     train         chosen         -0.526894   \n",
       "2   59149       -0.104880     train         chosen         -0.680847   \n",
       "3   20080        1.230862     train         chosen         -2.328661   \n",
       "4   72323        0.448723     train         chosen         -0.174440   \n",
       "..    ...             ...       ...            ...               ...   \n",
       "95  80580        1.554397     train         chosen          1.008231   \n",
       "96  52285        0.859001     train         chosen         -1.549616   \n",
       "97  50293        1.536220     train         chosen         -1.667574   \n",
       "98  32555       -1.131761     train         chosen         -1.406382   \n",
       "99  70805        0.145257     train         chosen          0.965101   \n",
       "\n",
       "   Preference_y_0  RewardDiff_0  PreferenceProb_0     id  RewardChosen_1  ...  \\\n",
       "0        rejected      0.286662          0.571179   1801       -1.198608  ...   \n",
       "1        rejected      1.939894          0.874340  87053        1.505481  ...   \n",
       "2        rejected      0.575967          0.640139  59149        0.134865  ...   \n",
       "3        rejected      3.559523          0.972335  20080        1.231286  ...   \n",
       "4        rejected      0.623163          0.650938  72323        0.188774  ...   \n",
       "..            ...           ...               ...    ...             ...  ...   \n",
       "95       rejected      0.546165          0.633245  80580        1.649850  ...   \n",
       "96       rejected      2.408617          0.917482  52285        0.817228  ...   \n",
       "97       rejected      3.203794          0.960977  50293        1.728942  ...   \n",
       "98       rejected      0.274621          0.568227  32555       -1.001117  ...   \n",
       "99       rejected     -0.819845          0.305797  70805       -0.155879  ...   \n",
       "\n",
       "   MaxPrefInterval Error   GT CrossEntropy  BrierScore  TooHighOrTooLow  \\\n",
       "0         0.174546   0.0  1.0     0.136626    0.190012              0.0   \n",
       "1         0.064916   0.0  1.0     0.011090    0.020059              0.0   \n",
       "2         0.121001   0.0  1.0     0.080880    0.122626              0.0   \n",
       "3         0.032511   0.0  1.0     0.000788    0.001535              0.0   \n",
       "4         0.193187   0.0  1.0     0.101546    0.148732              0.0   \n",
       "..             ...   ...  ...          ...         ...              ...   \n",
       "95        0.099772   0.0  1.0     0.070629    0.109103              0.0   \n",
       "96        0.075008   0.0  1.0     0.003974    0.007487              0.0   \n",
       "97        0.023557   0.0  1.0     0.000902    0.001754              0.0   \n",
       "98        0.129487   0.0  1.0     0.108164    0.156795              0.0   \n",
       "99        0.144874   1.0  1.0     0.537587    0.509695              0.0   \n",
       "\n",
       "    GDA_RwChosenAverage_Prob  GDA_RwChosenAverage_alpha  \\\n",
       "0                   0.217420                   0.217420   \n",
       "1                   0.223827                   0.223827   \n",
       "2                   0.372784                   0.372784   \n",
       "3                   0.252674                   0.252674   \n",
       "4                   0.369601                   0.369601   \n",
       "..                       ...                        ...   \n",
       "95                  0.146877                   0.146877   \n",
       "96                  0.289912                   0.289912   \n",
       "97                  0.141012                   0.141012   \n",
       "98                  0.189364                   0.189364   \n",
       "99                  0.372210                   0.372210   \n",
       "\n",
       "   GDA_RwRejectedAverage_Prob GDA_RwRejectedAverage_alpha  \n",
       "0                    0.281609                    0.281609  \n",
       "1                    0.351744                    0.351744  \n",
       "2                    0.353587                    0.353587  \n",
       "3                    0.119711                    0.119711  \n",
       "4                    0.342887                    0.342887  \n",
       "..                        ...                         ...  \n",
       "95                   0.170765                    0.170765  \n",
       "96                   0.236659                    0.236659  \n",
       "97                   0.231813                    0.231813  \n",
       "98                   0.227300                    0.227300  \n",
       "99                   0.151532                    0.151532  \n",
       "\n",
       "[100 rows x 321 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rewards.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b0ba05c-8878-41a9-97f9-8decadfc032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Cross Entropy for train: 0.04154004075548281\n",
      "Correlation Between Var(r1-r2) and CrossEntropy for train: -0.09365878075551694\n",
      "Correlation Between Var(p) and Error for train: 0.20166718376471401\n",
      "Correlation Between max(Var(r1), Var(r2)) and Cross Entropy for train: -0.013535195798154634\n",
      "Correlation Between and |p_max - p_min| and Error for train: 0.19557532967222443\n",
      "Correlation Between Var(r1-r2, *) and Cross Entropy for train: 0.0415400321483095\n",
      "Correlation Between TooHighTooLow and Cross Entropy for train: 0.03256358691461575\n",
      "Correlation Between GDA_Chosen_Prob and Cross Entropy for train: -0.06876185233774851\n",
      "Correlation Between GDA_Rejected_Prob and Cross Entropy for train: 0.0206490385863076\n",
      "Correlation Between GDA_Chosen_alpha and Cross Entropy for train: -0.06876185233774851\n",
      "Correlation Between GDA_Rejected_alpha and Cross Entropy for train: 0.0206490385863076\n",
      "Test Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Cross Entropy for test: 0.01696573905824865\n",
      "Correlation Between Var(r1-r2) and CrossEntropy for test: -0.10864997466387988\n",
      "Correlation Between Var(p) and Error for test: 0.12777914942833016\n",
      "Correlation Between max(Var(r1), Var(r2)) and Cross Entropy for test: -0.03576840148701882\n",
      "Correlation Between and |p_max - p_min| and Error for test: 0.12215235385201209\n",
      "Correlation Between Var(r1-r2, *) and Cross Entropy for test: 0.016966621773111475\n",
      "Correlation Between TooHighTooLow and Cross Entropy for test: 0.10736765676813996\n",
      "Correlation Between GDA_Chosen_Prob and Cross Entropy for test: -0.01256498992706344\n",
      "Correlation Between GDA_Rejected_Prob and Cross Entropy for test: -0.16724034044703706\n",
      "Correlation Between GDA_Chosen_alpha and Cross Entropy for test: -0.01256498992706344\n",
      "Correlation Between GDA_Rejected_alpha and Cross Entropy for test: -0.16724034044703706\n",
      "Eval Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Cross Entropy for eval: -0.00735990138875642\n",
      "Correlation Between Var(r1-r2) and CrossEntropy for eval: -0.0907103698705793\n",
      "Correlation Between Var(p) and Error for eval: 0.11840546813193538\n",
      "Correlation Between max(Var(r1), Var(r2)) and Cross Entropy for eval: -0.037660075751052687\n",
      "Correlation Between and |p_max - p_min| and Error for eval: 0.11230031764990536\n",
      "Correlation Between Var(r1-r2, *) and Cross Entropy for eval: -0.007355355125315216\n",
      "Correlation Between TooHighTooLow and Cross Entropy for eval: 0.10285063046111902\n",
      "Correlation Between GDA_Chosen_Prob and Cross Entropy for eval: 0.013719181136217897\n",
      "Correlation Between GDA_Rejected_Prob and Cross Entropy for eval: -0.17550801910447097\n",
      "Correlation Between GDA_Chosen_alpha and Cross Entropy for eval: 0.013719181136217897\n",
      "Correlation Between GDA_Rejected_alpha and Cross Entropy for eval: -0.17550801910447097\n",
      "OOD Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Cross Entropy for ood: 0.13664201566288164\n",
      "Correlation Between Var(r1-r2) and CrossEntropy for ood: -0.030348968922563276\n",
      "Correlation Between Var(p) and Error for ood: 0.12625329736560492\n",
      "Correlation Between max(Var(r1), Var(r2)) and Cross Entropy for ood: 0.10580599680610851\n",
      "Correlation Between and |p_max - p_min| and Error for ood: 0.11309957653359055\n",
      "Correlation Between Var(r1-r2, *) and Cross Entropy for ood: 0.1366409850465246\n",
      "Correlation Between TooHighTooLow and Cross Entropy for ood: 0.08439254880512294\n",
      "Correlation Between GDA_Chosen_Prob and Cross Entropy for ood: -0.20953269556489063\n",
      "Correlation Between GDA_Rejected_Prob and Cross Entropy for ood: 0.030047955217702375\n",
      "Correlation Between GDA_Chosen_alpha and Cross Entropy for ood: -0.20953269556489063\n",
      "Correlation Between GDA_Rejected_alpha and Cross Entropy for ood: 0.030047955217702375\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, eval_df, ood_df = split_dataset(final_rewards, \"Dataset_0\")\n",
    "# Compute Variances \n",
    "def compute_stats_ce_correlation(df, mode=\"train\", ensemble=True):\n",
    "    print(f\"Correlation Between Var(r1) + Var(r2) and Cross Entropy for {mode}: {df['RwVarianceSum'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "    print(f\"Correlation Between Var(r1-r2) and CrossEntropy for {mode}: {df['RwDiffVariance'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "    print(f\"Correlation Between Var(p) and Error for {mode}: {df['PrefProbVariance'].corr(df['Error'], method='spearman')}\")\n",
    "    print(f\"Correlation Between max(Var(r1), Var(r2)) and Cross Entropy for {mode}: {df['RewardMaxVariance'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "    print(f\"Correlation Between and |p_max - p_min| and Error for {mode}: {df['MaxPrefInterval'].corr(df['Error'], method='spearman')}\")\n",
    "    if ensemble:\n",
    "        print(f\"Correlation Between Var(r1-r2, *) and Cross Entropy for {mode}: {df['RwDiffDistinctPairs'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "        print(f\"Correlation Between TooHighTooLow and Cross Entropy for {mode}: {df['TooHighOrTooLow'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "        print(f\"Correlation Between GDA_Chosen_Prob and Cross Entropy for {mode}: {df['GDA_RwChosenAverage_Prob'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "        print(f\"Correlation Between GDA_Rejected_Prob and Cross Entropy for {mode}: {df['GDA_RwRejectedAverage_Prob'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "        print(f\"Correlation Between GDA_Chosen_alpha and Cross Entropy for {mode}: {df['GDA_RwChosenAverage_alpha'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "        print(f\"Correlation Between GDA_Rejected_alpha and Cross Entropy for {mode}: {df['GDA_RwRejectedAverage_alpha'].corr(df['CrossEntropy'], method='spearman')}\")\n",
    "        \n",
    "    \n",
    "print(\"Train Dataset\")\n",
    "compute_stats_ce_correlation(train_df, \"train\")\n",
    "print(\"Test Dataset\")\n",
    "compute_stats_ce_correlation(test_df, \"test\")\n",
    "print(\"Eval Dataset\")\n",
    "compute_stats_ce_correlation(eval_df, \"eval\")\n",
    "print(\"OOD Dataset\")\n",
    "compute_stats_ce_correlation(ood_df, \"ood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4486a92-b9f9-4fce-928a-c8059d33bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for train: 0.04154004075548281\n",
      "Correlation Between Var(r1-r2) and Brier Score for train: -0.09365878075551694\n",
      "Correlation Between Var(p) and Brier Score for train: 0.4840945267028453\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for train: -0.013535195798154634\n",
      "Correlation Between Var(r1-r2, *) and Brier Score for train: 0.0415400321483095\n",
      "Correlation Between TooHighTooLow and Brier Score for train: 0.03256358691461575\n",
      "Test Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for test: 0.01696573905824865\n",
      "Correlation Between Var(r1-r2) and Brier Score for test: -0.10864997466387988\n",
      "Correlation Between Var(p) and Brier Score for test: 0.322236670433129\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for test: -0.03576840148701882\n",
      "Correlation Between Var(r1-r2, *) and Brier Score for test: 0.016966621773111475\n",
      "Correlation Between TooHighTooLow and Brier Score for test: 0.10736765676813996\n",
      "Eval Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for eval: -0.00735990138875642\n",
      "Correlation Between Var(r1-r2) and Brier Score for eval: -0.0907103698705793\n",
      "Correlation Between Var(p) and Brier Score for eval: 0.30988203179789975\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for eval: -0.037660075751052687\n",
      "Correlation Between Var(r1-r2, *) and Brier Score for eval: -0.007355355125315216\n",
      "Correlation Between TooHighTooLow and Brier Score for eval: 0.10285063046111902\n",
      "OOD Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for ood: 0.13664201566288164\n",
      "Correlation Between Var(r1-r2) and Brier Score for ood: -0.030348968922563276\n",
      "Correlation Between Var(p) and Brier Score for ood: 0.22402185570435587\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for ood: 0.10580599680610851\n",
      "Correlation Between Var(r1-r2, *) and Brier Score for ood: 0.1366409850465246\n",
      "Correlation Between TooHighTooLow and Brier Score for ood: 0.08439254880512294\n"
     ]
    }
   ],
   "source": [
    "# Compute Variances \n",
    "def compute_stats_brier_correlation(df, mode=\"train\", ensemble=True):\n",
    "    print(f\"Correlation Between Var(r1) + Var(r2) and Brier Score for {mode}: {df['RwVarianceSum'].corr(df['BrierScore'], method='spearman')}\")\n",
    "    print(f\"Correlation Between Var(r1-r2) and Brier Score for {mode}: {df['RwDiffVariance'].corr(df['BrierScore'], method='spearman')}\")\n",
    "    print(f\"Correlation Between Var(p) and Brier Score for {mode}: {df['PrefProbVariance'].corr(df['BrierScore'], method='spearman')}\")\n",
    "    print(f\"Correlation Between max(Var(r1), Var(r2)) and Brier Score for {mode}: {df['RewardMaxVariance'].corr(df['BrierScore'], method='spearman')}\")\n",
    "    if ensemble:\n",
    "        print(f\"Correlation Between Var(r1-r2, *) and Brier Score for {mode}: {df['RwDiffDistinctPairs'].corr(df['BrierScore'], method='spearman')}\")\n",
    "        print(f\"Correlation Between TooHighTooLow and Brier Score for {mode}: {df['TooHighOrTooLow'].corr(df['BrierScore'], method='spearman')}\")\n",
    "    \n",
    "print(\"Train Dataset\")\n",
    "compute_stats_brier_correlation(train_df, \"train\")\n",
    "print(\"Test Dataset\")\n",
    "compute_stats_brier_correlation(test_df, \"test\")\n",
    "print(\"Eval Dataset\")\n",
    "compute_stats_brier_correlation(eval_df, \"eval\")\n",
    "print(\"OOD Dataset\")\n",
    "compute_stats_brier_correlation(ood_df, \"ood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41fc31-3181-45cb-9dee-c143b62c3890",
   "metadata": {},
   "source": [
    "# Baseline 2: Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbba159e-4d0f-4771-9ab0-f6033f8da45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpo_df = pd.read_csv('/users/lucelo/UQLRM/metadata_vpo.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a23d06a5-1349-4ce1-9536-1958a090bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_df = vpo_df[vpo_df['Preference'] == 'chosen'].rename(columns={'RewardScoreMean': 'RewardChosenMean', 'RewardScoreVar': 'RewardChosenVar'})\n",
    "rejected_df = vpo_df[vpo_df['Preference'] == 'rejected'].rename(columns={'RewardScoreMean': 'RewardRejectedMean', 'RewardScoreVar': 'RewardRejectedVar'})\n",
    "final_vpo_df = chosen_df.merge(rejected_df, on=['id', 'Dataset'], how='inner')\n",
    "final_vpo_df['RewardDiff'] = final_vpo_df['RewardChosenMean'] - final_vpo_df['RewardRejectedMean']\n",
    "final_vpo_df['PrefProbAverage'] = sigmoid(final_vpo_df['RewardDiff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1239816-b379-4897-99db-04625149dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rw_diff_var(x):\n",
    "    chosen_mu = x['RewardChosenMean']\n",
    "    chosen_var = x['RewardChosenVar']\n",
    "    chosen_points = np.random.normal(chosen_mu, np.sqrt(chosen_var), 1000)\n",
    "\n",
    "    rejected_mu = x['RewardRejectedMean']\n",
    "    rejected_var = x['RewardRejectedVar']\n",
    "    rejected_points = np.random.normal(rejected_mu, np.sqrt(rejected_var), 1000)\n",
    "\n",
    "    diff = chosen_points - rejected_points\n",
    "    probs = sigmoid(diff)\n",
    "    return pd.Series([np.mean(diff), np.var(diff), np.mean(probs), np.var(probs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ba4ee85-212d-444e-9fc5-dc87e5920b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preference_x</th>\n",
       "      <th>RewardChosenMean</th>\n",
       "      <th>RewardChosenVar</th>\n",
       "      <th>Model_x</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>Preference_y</th>\n",
       "      <th>RewardRejectedMean</th>\n",
       "      <th>RewardRejectedVar</th>\n",
       "      <th>Model_y</th>\n",
       "      <th>RewardDiff</th>\n",
       "      <th>PrefProbAverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chosen</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.398438</td>\n",
       "      <td>0.459625</td>\n",
       "      <td>vpo</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.649356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.054688</td>\n",
       "      <td>0.461424</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>vpo</td>\n",
       "      <td>-1.176758</td>\n",
       "      <td>0.235636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>0.450735</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.457833</td>\n",
       "      <td>vpo</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.501953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.054688</td>\n",
       "      <td>0.461424</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.445312</td>\n",
       "      <td>0.447228</td>\n",
       "      <td>vpo</td>\n",
       "      <td>-0.609375</td>\n",
       "      <td>0.352202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.054688</td>\n",
       "      <td>0.461424</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>0.454270</td>\n",
       "      <td>vpo</td>\n",
       "      <td>-0.601562</td>\n",
       "      <td>0.353986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Preference_x  RewardChosenMean  RewardChosenVar Model_x Dataset  id  \\\n",
       "0       chosen          0.217773         0.474215     vpo   train   0   \n",
       "1       chosen         -1.054688         0.461424     vpo   train   1   \n",
       "2       chosen         -0.306641         0.450735     vpo   train   2   \n",
       "3       chosen         -1.054688         0.461424     vpo   train   3   \n",
       "4       chosen         -1.054688         0.461424     vpo   train   4   \n",
       "\n",
       "  Preference_y  RewardRejectedMean  RewardRejectedVar Model_y  RewardDiff  \\\n",
       "0     rejected           -0.398438           0.459625     vpo    0.616211   \n",
       "1     rejected            0.122070           0.474215     vpo   -1.176758   \n",
       "2     rejected           -0.314453           0.457833     vpo    0.007812   \n",
       "3     rejected           -0.445312           0.447228     vpo   -0.609375   \n",
       "4     rejected           -0.453125           0.454270     vpo   -0.601562   \n",
       "\n",
       "   PrefProbAverage  \n",
       "0         0.649356  \n",
       "1         0.235636  \n",
       "2         0.501953  \n",
       "3         0.352202  \n",
       "4         0.353986  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vpo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24ec9abf-4273-4788-b1cc-c7d1756a397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unc_stats_vi(vpo_df):\n",
    "    # 4. Reward Diff (r_chosen - r_rejected) Statistics\n",
    "    vpo_df[['RwDiffAverage', 'RwDiffVariance', 'PrefProbAverage', 'PrefProbVariance']] = vpo_df.apply(lambda x: compute_rw_diff_var(x), axis=1)\n",
    "    \n",
    "    # 5. Variance Sum = Var(r_chosen) + Var(r_rejected)\n",
    "    vpo_df['RwVarianceSum'] = vpo_df['RewardChosenVar'] + vpo_df['RewardRejectedVar']\n",
    "    \n",
    "    # 8. Max Variance = max(Var(r_chosen), Var(r_rejected))\n",
    "    vpo_df['RewardMaxVariance'] = vpo_df[['RewardChosenVar', 'RewardRejectedVar']].max(axis=1)\n",
    "\n",
    "compute_unc_stats_vi(final_vpo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48fb29f5-36a0-47ce-b302-3365aa4dd822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preference_x</th>\n",
       "      <th>RewardChosenMean</th>\n",
       "      <th>RewardChosenVar</th>\n",
       "      <th>Model_x</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>Preference_y</th>\n",
       "      <th>RewardRejectedMean</th>\n",
       "      <th>RewardRejectedVar</th>\n",
       "      <th>Model_y</th>\n",
       "      <th>RewardDiff</th>\n",
       "      <th>PrefProbAverage</th>\n",
       "      <th>RwDiffAverage</th>\n",
       "      <th>RwDiffVariance</th>\n",
       "      <th>PrefProbVariance</th>\n",
       "      <th>RwVarianceSum</th>\n",
       "      <th>RewardMaxVariance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chosen</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.398438</td>\n",
       "      <td>0.459625</td>\n",
       "      <td>vpo</td>\n",
       "      <td>0.616211</td>\n",
       "      <td>0.628106</td>\n",
       "      <td>0.636186</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.038298</td>\n",
       "      <td>0.933841</td>\n",
       "      <td>0.474215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.054688</td>\n",
       "      <td>0.461424</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0.122070</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>vpo</td>\n",
       "      <td>-1.176758</td>\n",
       "      <td>0.270729</td>\n",
       "      <td>-1.171465</td>\n",
       "      <td>0.903217</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.935640</td>\n",
       "      <td>0.474215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>0.450735</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.457833</td>\n",
       "      <td>vpo</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.509090</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.038884</td>\n",
       "      <td>0.908569</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.054688</td>\n",
       "      <td>0.461424</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.445312</td>\n",
       "      <td>0.447228</td>\n",
       "      <td>vpo</td>\n",
       "      <td>-0.609375</td>\n",
       "      <td>0.377058</td>\n",
       "      <td>-0.590879</td>\n",
       "      <td>0.835134</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.908652</td>\n",
       "      <td>0.461424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chosen</td>\n",
       "      <td>-1.054688</td>\n",
       "      <td>0.461424</td>\n",
       "      <td>vpo</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>rejected</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>0.454270</td>\n",
       "      <td>vpo</td>\n",
       "      <td>-0.601562</td>\n",
       "      <td>0.385717</td>\n",
       "      <td>-0.552088</td>\n",
       "      <td>0.878495</td>\n",
       "      <td>0.036054</td>\n",
       "      <td>0.915695</td>\n",
       "      <td>0.461424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Preference_x  RewardChosenMean  RewardChosenVar Model_x Dataset  id  \\\n",
       "0       chosen          0.217773         0.474215     vpo   train   0   \n",
       "1       chosen         -1.054688         0.461424     vpo   train   1   \n",
       "2       chosen         -0.306641         0.450735     vpo   train   2   \n",
       "3       chosen         -1.054688         0.461424     vpo   train   3   \n",
       "4       chosen         -1.054688         0.461424     vpo   train   4   \n",
       "\n",
       "  Preference_y  RewardRejectedMean  RewardRejectedVar Model_y  RewardDiff  \\\n",
       "0     rejected           -0.398438           0.459625     vpo    0.616211   \n",
       "1     rejected            0.122070           0.474215     vpo   -1.176758   \n",
       "2     rejected           -0.314453           0.457833     vpo    0.007812   \n",
       "3     rejected           -0.445312           0.447228     vpo   -0.609375   \n",
       "4     rejected           -0.453125           0.454270     vpo   -0.601562   \n",
       "\n",
       "   PrefProbAverage  RwDiffAverage  RwDiffVariance  PrefProbVariance  \\\n",
       "0         0.628106       0.636186        0.971300          0.038298   \n",
       "1         0.270729      -1.171465        0.903217          0.027886   \n",
       "2         0.509090       0.039460        0.870322          0.038884   \n",
       "3         0.377058      -0.590879        0.835134          0.034991   \n",
       "4         0.385717      -0.552088        0.878495          0.036054   \n",
       "\n",
       "   RwVarianceSum  RewardMaxVariance  \n",
       "0       0.933841           0.474215  \n",
       "1       0.935640           0.474215  \n",
       "2       0.908569           0.457833  \n",
       "3       0.908652           0.461424  \n",
       "4       0.915695           0.461424  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vpo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2dee467-e703-4e83-9be9-8daf415c24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_ce(final_vpo_df)\n",
    "vpo_train_df, vpo_test_df, vpo_eval_df, vpo_ood_df = split_dataset(final_vpo_df, \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ce705a0-7dba-48aa-beb5-bac7fceca9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Cross Entropy for train: -0.07263562129560804\n",
      "Correlation Between Var(r1-r2) and Error for train: -0.017003221343124954\n",
      "Correlation Between Var(p) and Cross Entropy for train: 0.6607520541665648\n",
      "Correlation Between max(Var(r1), Var(r2)) and Cross Entropy for train: -0.09117890920300704\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MaxPrefInterval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/scratch-ssd/lucelo/conda_envs/uqrm/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MaxPrefInterval'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcompute_stats_ce_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvpo_train_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m compute_stats_ce_correlation(vpo_test_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensemble\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mcompute_stats_ce_correlation\u001b[0;34m(df, mode, ensemble)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation Between Var(p) and Cross Entropy for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrefProbVariance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrossEntropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mmethod\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation Between max(Var(r1), Var(r2)) and Cross Entropy for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRewardMaxVariance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrossEntropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mmethod\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation Between and |p_max - p_min| and Error for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxPrefInterval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mmethod\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensemble:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation Between Var(r1-r2, *) and Cross Entropy for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRwDiffDistinctPairs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcorr(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrossEntropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mmethod\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspearman\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch-ssd/lucelo/conda_envs/uqrm/lib/python3.9/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/scratch-ssd/lucelo/conda_envs/uqrm/lib/python3.9/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MaxPrefInterval'"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset\")\n",
    "compute_stats_ce_correlation(vpo_train_df, \"train\", ensemble=False)\n",
    "print(\"Test Dataset\")\n",
    "compute_stats_ce_correlation(vpo_test_df, \"test\", ensemble=False)\n",
    "print(\"Eval Dataset\")\n",
    "compute_stats_ce_correlation(vpo_eval_df, \"eval\", ensemble=False)\n",
    "print(\"OOD Dataset\")\n",
    "compute_stats_ce_correlation(vpo_ood_df, \"ood\", ensemble=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0876b67c-0320-4160-bc4d-d84f3ac716cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for train: -0.07242801215116604\n",
      "Correlation Between Var(r1-r2) and Brier Score for train: -0.03854055980902226\n",
      "Correlation Between Var(p) and Brier Score for train: 0.6605964905804889\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for train: -0.09096605762816938\n",
      "Test Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for test: -0.0018904309537869208\n",
      "Correlation Between Var(r1-r2) and Brier Score for test: 0.0016976488094075535\n",
      "Correlation Between Var(p) and Brier Score for test: 0.4784675079243519\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for test: -0.023769930365107186\n",
      "Eval Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for eval: -0.03186996336372765\n",
      "Correlation Between Var(r1-r2) and Brier Score for eval: -0.015365901395559713\n",
      "Correlation Between Var(p) and Brier Score for eval: 0.45071649727010993\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for eval: -0.05165885539659077\n",
      "OOD Dataset\n",
      "Correlation Between Var(r1) + Var(r2) and Brier Score for ood: 0.06753482263737323\n",
      "Correlation Between Var(r1-r2) and Brier Score for ood: 0.06444949044309535\n",
      "Correlation Between Var(p) and Brier Score for ood: 0.3826018582161374\n",
      "Correlation Between max(Var(r1), Var(r2)) and Brier Score for ood: 0.06710059050277874\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset\")\n",
    "compute_stats_brier_correlation(vpo_train_df, \"train\", ensemble=False)\n",
    "print(\"Test Dataset\")\n",
    "compute_stats_brier_correlation(vpo_test_df, \"test\", ensemble=False)\n",
    "print(\"Eval Dataset\")\n",
    "compute_stats_brier_correlation(vpo_eval_df, \"eval\", ensemble=False)\n",
    "print(\"OOD Dataset\")\n",
    "compute_stats_brier_correlation(vpo_ood_df, \"ood\", ensemble=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffd459-0093-4b59-a79c-934e5533134b",
   "metadata": {},
   "source": [
    "# Baseline 3: Finetuned Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc9f5f6-a2d1-4d01-9fe8-0c53f9753e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(df):\n",
    "    return df.apply(lambda row: entropy(row), axis=1)\n",
    "    \n",
    "def compute_uncertanties(dfs):\n",
    "    # Compute single model entropies\n",
    "    for df in dfs:\n",
    "        df['entropy'] = compute_entropy(df[['First', 'Second']])\n",
    "    \n",
    "    \n",
    "    for i, df in enumerate(dfs):\n",
    "        # Add unique suffixes to the column names\n",
    "        df.columns = [f\"{col}_{i}\" if col != 'id' else col for col in df.columns]\n",
    "\n",
    "    # Use reduce to merge all dataframes\n",
    "    from functools import reduce\n",
    "    final_df = reduce(lambda left,right: pd.merge(left,right,on='id'), dfs)\n",
    "\n",
    "    first_cols = [col for col in final_df.columns if 'First_' in col]\n",
    "    second_cols = [col for col in final_df.columns if 'Second_' in col]\n",
    "    entropy_cols = [col for col in final_df.columns if 'entropy_' in col]\n",
    "\n",
    "    avg_first = final_df[first_cols].mean(axis=1)\n",
    "    avg_second = final_df[second_cols].mean(axis=1)\n",
    "    avg_entropy = final_df[entropy_cols].mean(axis=1)\n",
    "    var_first = final_df[first_cols].var(axis=1)\n",
    "    avg_df = pd.concat([avg_first, avg_second, avg_entropy, var_first], axis=1)\n",
    "    avg_df.columns = ['First', 'Second', 'Aleatoric Uncertainty', 'Variance']\n",
    "\n",
    "    \n",
    "    avg_df['Predictive Uncertainty'] = compute_entropy(avg_df[['First', 'Second']])\n",
    "    avg_df['Epistemic Uncertainty'] = avg_df['Predictive Uncertainty'] - avg_df['Aleatoric Uncertainty']\n",
    "    return avg_df['Epistemic Uncertainty'], avg_df['Predictive Uncertainty'], avg_df['Aleatoric Uncertainty'], avg_df[['First', 'Second']], avg_df['Variance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2b2c5c4-f4d1-4434-8508-b846ff320e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /scratch-ssd/oatml/huggingface/token\n",
      "Login successful\n",
      "Number of ensemble predictions loaded: 8\n",
      "Number of ensemble predictions loaded: 8\n",
      "Number of ensemble predictions loaded: 8\n",
      "Number of ensemble predictions loaded: 8\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HUGGINGFACE_WRITETOKEN\n",
    "def load_predictions(exp_prefix, name, checkpoint, mode, ensemble_size, active_learning=False):\n",
    "    ensemble_df = []\n",
    "    for j in range(ensemble_size):\n",
    "        if active_learning:\n",
    "            datafile = os.path.join(exp_prefix, f\"{name}\", \"predictions\", f\"{name}_{j}\", f\"checkpoint-{i}\", f\"eval_{mode}\", \"predictions.csv\")\n",
    "        else:\n",
    "            datafile = os.path.join(exp_prefix, f\"{name}_{j}\", f\"{name}_{j}\", f\"checkpoint-{i}\", f\"eval_{mode}\", \"predictions.csv\")\n",
    "        df = load_dataset(\"luckeciano/uqlrm_predictions\", data_files=datafile)['train'].to_pandas()\n",
    "        ensemble_df.append(df)\n",
    "\n",
    "\n",
    "    print(f\"Number of ensemble predictions loaded: {len(ensemble_df)}\")\n",
    "    epistemic, predictive, aleatoric, ens_predictions, var_predictions = compute_uncertanties(ensemble_df)\n",
    "    return ens_predictions, var_predictions\n",
    "    \n",
    "exp_prefix = \"scratch/lucelo/sft/results/\"\n",
    "name = \"gpt2_rwft_reddit_1\"\n",
    "i = 80\n",
    "train_ens_preds, train_var_preds = load_predictions(exp_prefix, name, i, \"train\", 8)\n",
    "test_ens_preds, test_var_preds = load_predictions(exp_prefix, name, i, \"test\", 8)\n",
    "eval_ens_preds, eval_var_preds = load_predictions(exp_prefix, name, i, \"eval\", 8)\n",
    "ood_ens_preds, ood_var_preds = load_predictions(exp_prefix, name, i, \"ood\", 8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fe1a230-3894-4bf9-9f69-1c64193aaf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ce_ens(ens_preds, var_preds):\n",
    "    finetune_ens_df = pd.DataFrame()\n",
    "    finetune_ens_df['PrefProbAverage'] = ens_preds['First']\n",
    "    finetune_ens_df['PrefProbVariance'] = var_preds\n",
    "    finetune_ens_df['Error'] = (finetune_ens_df['PrefProbAverage'] < 0.5) * 1.0\n",
    "    finetune_ens_df['GT'] = 1.0\n",
    "    finetune_ens_df['CrossEntropy'] = kl_div(finetune_ens_df['GT'], finetune_ens_df['PrefProbAverage'])\n",
    "    finetune_ens_df['BrierScore'] = brier_score_loss(finetune_ens_df['GT'], finetune_ens_df['PrefProbAverage'])\n",
    "    return finetune_ens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e47355-c0c8-4a86-818e-92d318b79a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Between Var(p) and Cross Entropy for Training: 0.056321783544015507\n",
      "Correlation Between Var(p) and Cross Entropy for Training: 0.07439719985948799\n",
      "Correlation Between Var(p) and Cross Entropy for Training: 0.05166463652945645\n",
      "Correlation Between Var(p) and Cross Entropy for Training: -0.07087779592929311\n"
     ]
    }
   ],
   "source": [
    "train_df = compute_ce_ens(train_ens_preds, train_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {train_df['PrefProbVariance'].corr(train_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "test_df = compute_ce_ens(test_ens_preds, test_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {test_df['PrefProbVariance'].corr(test_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "eval_df = compute_ce_ens(eval_ens_preds, eval_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {eval_df['PrefProbVariance'].corr(eval_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "ood_df = compute_ce_ens(ood_ens_preds, ood_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {ood_df['PrefProbVariance'].corr(ood_df['CrossEntropy'], method='spearman')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e34ce0-e505-4927-80e0-468d4e6d6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {train_df['PrefProbVariance'].corr(train_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {test_df['PrefProbVariance'].corr(test_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {eval_df['PrefProbVariance'].corr(eval_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {ood_df['PrefProbVariance'].corr(ood_df['BrierScore'], method='spearman')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f5068e9-b3d5-4e98-bb9e-6b0ecf75488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ensemble predictions loaded: 8\n",
      "Number of ensemble predictions loaded: 8\n",
      "Number of ensemble predictions loaded: 8\n",
      "Number of ensemble predictions loaded: 8\n"
     ]
    }
   ],
   "source": [
    "exp_prefix = \"scratch/lucelo/active_learning/results/\"\n",
    "name = \"al_ep_v11_3\"\n",
    "i = 60\n",
    "train_ens_preds, train_var_preds = load_predictions(exp_prefix, name, i, \"train\", 8, active_learning=True)\n",
    "test_ens_preds, test_var_preds = load_predictions(exp_prefix, name, i, \"test\", 8, active_learning=True)\n",
    "eval_ens_preds, eval_var_preds = load_predictions(exp_prefix, name, i, \"eval\", 8, active_learning=True)\n",
    "ood_ens_preds, ood_var_preds = load_predictions(exp_prefix, name, i, \"ood\", 8, active_learning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78e626f1-c7f1-4d89-88c2-739891bfa615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Between Var(p) and Cross Entropy for Training: -0.09131798391076847\n",
      "Correlation Between Var(p) and Cross Entropy for Training: -0.014102754498991945\n",
      "Correlation Between Var(p) and Cross Entropy for Training: 0.006720589136532155\n",
      "Correlation Between Var(p) and Cross Entropy for Training: -0.11646054855009284\n"
     ]
    }
   ],
   "source": [
    "train_df = compute_ce_ens(train_ens_preds, train_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {train_df['PrefProbVariance'].corr(train_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "test_df = compute_ce_ens(test_ens_preds, test_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {test_df['PrefProbVariance'].corr(test_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "eval_df = compute_ce_ens(eval_ens_preds, eval_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {eval_df['PrefProbVariance'].corr(eval_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "ood_df = compute_ce_ens(ood_ens_preds, ood_var_preds)\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {ood_df['PrefProbVariance'].corr(ood_df['CrossEntropy'], method='spearman')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41865b7c-a277-4ba0-8fe3-86e2f37ff32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {train_df['PrefProbVariance'].corr(train_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {test_df['PrefProbVariance'].corr(test_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {eval_df['PrefProbVariance'].corr(eval_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Brier Score for Training: {ood_df['PrefProbVariance'].corr(ood_df['BrierScore'], method='spearman')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc44d17-8020-4398-811f-4d9e4d061007",
   "metadata": {},
   "source": [
    "# Baseline 4: Finetuned Ensembles with Different Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1db15aa-4c61-4d7c-a09b-f3592439dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rw_set(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=0)[['id', 'RewardScore', 'Dataset', 'Preference']]\n",
    "    chosen_df = df[df['Preference'] == 'chosen'].rename(columns={'RewardScore': 'RewardChosen'})\n",
    "    rejected_df = df[df['Preference'] == 'rejected'].rename(columns={'RewardScore': 'RewardRejected'})\n",
    "    merged_df = chosen_df.merge(rejected_df, on=['id', 'Dataset'], how='inner')\n",
    "    merged_df['RewardDiff'] = merged_df['RewardChosen'] - merged_df['RewardRejected']\n",
    "    merged_df['PreferenceProb'] = sigmoid(merged_df['RewardDiff'])\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "687c1c0d-9794-4e7c-bbeb-37e1504dcbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2xl_df = generate_rw_set('/users/lucelo/UQLRM/uqlrm/scripts/slurm/metadata_gpt2xl-infer_.tsv')[['id', 'Dataset', 'RewardChosen', 'RewardRejected', 'RewardDiff', 'PreferenceProb']]\n",
    "gpt2xl_df.columns = [f\"{col}_gpt2xl\" if (col != 'id' and col != 'Dataset') else col for col in gpt2xl_df.columns]\n",
    "llama_df = generate_rw_set('/users/lucelo/UQLRM/uqlrm/scripts/slurm/metadata_llama_rw_infer_v0_.tsv')[['id', 'Dataset', 'RewardChosen', 'RewardRejected', 'RewardDiff', 'PreferenceProb']]\n",
    "llama_df.columns = [f\"{col}_llama\" if (col != 'id' and col != 'Dataset') else col for col in llama_df.columns]\n",
    "gpt2_df = generate_rw_set('/users/lucelo/UQLRM/metadata_gpt2-after-reward-modeling.tsv')[['id', 'Dataset', 'RewardChosen', 'RewardRejected', 'RewardDiff', 'PreferenceProb']]\n",
    "gpt2_df.columns = [f\"{col}_gpt2\" if (col != 'id' and col != 'Dataset') else col for col in gpt2_df.columns]\n",
    "hermes_df = generate_rw_set('/users/lucelo/UQLRM/metadata_single_mlp_0.tsv')[['id', 'Dataset', 'RewardChosen', 'RewardRejected', 'RewardDiff', 'PreferenceProb']]\n",
    "hermes_df.columns = [f\"{col}_hermes\" if (col != 'id' and col != 'Dataset') else col for col in hermes_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd47c4c3-230b-4586-a40f-d818c1e84530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>RewardChosen_gpt2xl</th>\n",
       "      <th>RewardRejected_gpt2xl</th>\n",
       "      <th>RewardDiff_gpt2xl</th>\n",
       "      <th>PreferenceProb_gpt2xl</th>\n",
       "      <th>RewardChosen_llama</th>\n",
       "      <th>RewardRejected_llama</th>\n",
       "      <th>RewardDiff_llama</th>\n",
       "      <th>PreferenceProb_llama</th>\n",
       "      <th>RewardChosen_hermes</th>\n",
       "      <th>RewardRejected_hermes</th>\n",
       "      <th>RewardDiff_hermes</th>\n",
       "      <th>PreferenceProb_hermes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801</td>\n",
       "      <td>train</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.578125</td>\n",
       "      <td>-0.203125</td>\n",
       "      <td>0.449393</td>\n",
       "      <td>1.046875</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>1.265625</td>\n",
       "      <td>0.779993</td>\n",
       "      <td>-1.021655</td>\n",
       "      <td>-1.308317</td>\n",
       "      <td>0.286662</td>\n",
       "      <td>0.571179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87053</td>\n",
       "      <td>train</td>\n",
       "      <td>1.539062</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>2.890625</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>2.382812</td>\n",
       "      <td>0.915507</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>-0.526894</td>\n",
       "      <td>1.939894</td>\n",
       "      <td>0.874340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59149</td>\n",
       "      <td>train</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.117188</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.697609</td>\n",
       "      <td>-0.628906</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.491212</td>\n",
       "      <td>-0.104880</td>\n",
       "      <td>-0.680847</td>\n",
       "      <td>0.575967</td>\n",
       "      <td>0.640139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080</td>\n",
       "      <td>train</td>\n",
       "      <td>1.515625</td>\n",
       "      <td>-0.351562</td>\n",
       "      <td>1.867188</td>\n",
       "      <td>0.866133</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.482422</td>\n",
       "      <td>1.267578</td>\n",
       "      <td>0.780328</td>\n",
       "      <td>1.230862</td>\n",
       "      <td>-2.328661</td>\n",
       "      <td>3.559523</td>\n",
       "      <td>0.972335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72323</td>\n",
       "      <td>train</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.611382</td>\n",
       "      <td>2.218750</td>\n",
       "      <td>1.554688</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.660172</td>\n",
       "      <td>0.448723</td>\n",
       "      <td>-0.174440</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.650938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id Dataset  RewardChosen_gpt2xl  RewardRejected_gpt2xl  \\\n",
       "0   1801   train             2.375000               2.578125   \n",
       "1  87053   train             1.539062               0.636719   \n",
       "2  59149   train             1.953125               1.117188   \n",
       "3  20080   train             1.515625              -0.351562   \n",
       "4  72323   train             0.960938               0.507812   \n",
       "\n",
       "   RewardDiff_gpt2xl  PreferenceProb_gpt2xl  RewardChosen_llama  \\\n",
       "0          -0.203125               0.449393            1.046875   \n",
       "1           0.902344               0.711431            2.890625   \n",
       "2           0.835938               0.697609           -0.628906   \n",
       "3           1.867188               0.866133            1.750000   \n",
       "4           0.453125               0.611382            2.218750   \n",
       "\n",
       "   RewardRejected_llama  RewardDiff_llama  PreferenceProb_llama  \\\n",
       "0             -0.218750          1.265625              0.779993   \n",
       "1              0.507812          2.382812              0.915507   \n",
       "2             -0.593750         -0.035156              0.491212   \n",
       "3              0.482422          1.267578              0.780328   \n",
       "4              1.554688          0.664062              0.660172   \n",
       "\n",
       "   RewardChosen_hermes  RewardRejected_hermes  RewardDiff_hermes  \\\n",
       "0            -1.021655              -1.308317           0.286662   \n",
       "1             1.413000              -0.526894           1.939894   \n",
       "2            -0.104880              -0.680847           0.575967   \n",
       "3             1.230862              -2.328661           3.559523   \n",
       "4             0.448723              -0.174440           0.623163   \n",
       "\n",
       "   PreferenceProb_hermes  \n",
       "0               0.571179  \n",
       "1               0.874340  \n",
       "2               0.640139  \n",
       "3               0.972335  \n",
       "4               0.650938  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df = pd.merge(gpt2xl_df, llama_df, on=['id', 'Dataset'], how='inner')\n",
    "# joined_df = pd.merge(joined_df, gpt2_df, on=['id', 'Dataset'], how='inner')\n",
    "joined_df = pd.merge(joined_df, hermes_df, on=['id', 'Dataset'], how='inner')\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eab7d0c-ee7f-4ee4-8340-909e0357ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df['PrefProbVariance'] = joined_df.filter(like=\"PreferenceProb\", axis=1).var(axis=1)\n",
    "joined_df['PrefProbAverage'] = joined_df.filter(like=\"PreferenceProb\", axis=1).mean(axis=1)\n",
    "joined_df['RwDiffVariance'] = joined_df.filter(like=\"RewardDiff\", axis=1).var(axis=1)\n",
    "joined_df['GT'] = 1.0\n",
    "joined_df['Error'] = (joined_df['PrefProbAverage'] < 0.5) * 1.0\n",
    "joined_df['CrossEntropy'] = kl_div(joined_df['GT'], joined_df['PrefProbAverage'])\n",
    "joined_df['BrierScore'] = brier_score_loss(joined_df['GT'], joined_df['PrefProbAverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f16d222-553d-49df-9041-1a878255cd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>RewardChosen_gpt2xl</th>\n",
       "      <th>RewardRejected_gpt2xl</th>\n",
       "      <th>RewardDiff_gpt2xl</th>\n",
       "      <th>PreferenceProb_gpt2xl</th>\n",
       "      <th>RewardChosen_llama</th>\n",
       "      <th>RewardRejected_llama</th>\n",
       "      <th>RewardDiff_llama</th>\n",
       "      <th>PreferenceProb_llama</th>\n",
       "      <th>RewardChosen_hermes</th>\n",
       "      <th>RewardRejected_hermes</th>\n",
       "      <th>RewardDiff_hermes</th>\n",
       "      <th>PreferenceProb_hermes</th>\n",
       "      <th>PrefProbVariance</th>\n",
       "      <th>PrefProbAverage</th>\n",
       "      <th>RwDiffVariance</th>\n",
       "      <th>GT</th>\n",
       "      <th>Error</th>\n",
       "      <th>CrossEntropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801</td>\n",
       "      <td>train</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.578125</td>\n",
       "      <td>-0.203125</td>\n",
       "      <td>0.449393</td>\n",
       "      <td>1.046875</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>1.265625</td>\n",
       "      <td>0.779993</td>\n",
       "      <td>-1.021655</td>\n",
       "      <td>-1.308317</td>\n",
       "      <td>0.286662</td>\n",
       "      <td>0.571179</td>\n",
       "      <td>0.027955</td>\n",
       "      <td>0.600188</td>\n",
       "      <td>0.559248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87053</td>\n",
       "      <td>train</td>\n",
       "      <td>1.539062</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>2.890625</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>2.382812</td>\n",
       "      <td>0.915507</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>-0.526894</td>\n",
       "      <td>1.939894</td>\n",
       "      <td>0.874340</td>\n",
       "      <td>0.011647</td>\n",
       "      <td>0.833760</td>\n",
       "      <td>0.577412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59149</td>\n",
       "      <td>train</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>1.117188</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.697609</td>\n",
       "      <td>-0.628906</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.491212</td>\n",
       "      <td>-0.104880</td>\n",
       "      <td>-0.680847</td>\n",
       "      <td>0.575967</td>\n",
       "      <td>0.640139</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>0.609653</td>\n",
       "      <td>0.199977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080</td>\n",
       "      <td>train</td>\n",
       "      <td>1.515625</td>\n",
       "      <td>-0.351562</td>\n",
       "      <td>1.867188</td>\n",
       "      <td>0.866133</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.482422</td>\n",
       "      <td>1.267578</td>\n",
       "      <td>0.780328</td>\n",
       "      <td>1.230862</td>\n",
       "      <td>-2.328661</td>\n",
       "      <td>3.559523</td>\n",
       "      <td>0.972335</td>\n",
       "      <td>0.009251</td>\n",
       "      <td>0.872932</td>\n",
       "      <td>1.412757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72323</td>\n",
       "      <td>train</td>\n",
       "      <td>0.960938</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.611382</td>\n",
       "      <td>2.218750</td>\n",
       "      <td>1.554688</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.660172</td>\n",
       "      <td>0.448723</td>\n",
       "      <td>-0.174440</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.650938</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.640831</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id Dataset  RewardChosen_gpt2xl  RewardRejected_gpt2xl  \\\n",
       "0   1801   train             2.375000               2.578125   \n",
       "1  87053   train             1.539062               0.636719   \n",
       "2  59149   train             1.953125               1.117188   \n",
       "3  20080   train             1.515625              -0.351562   \n",
       "4  72323   train             0.960938               0.507812   \n",
       "\n",
       "   RewardDiff_gpt2xl  PreferenceProb_gpt2xl  RewardChosen_llama  \\\n",
       "0          -0.203125               0.449393            1.046875   \n",
       "1           0.902344               0.711431            2.890625   \n",
       "2           0.835938               0.697609           -0.628906   \n",
       "3           1.867188               0.866133            1.750000   \n",
       "4           0.453125               0.611382            2.218750   \n",
       "\n",
       "   RewardRejected_llama  RewardDiff_llama  PreferenceProb_llama  \\\n",
       "0             -0.218750          1.265625              0.779993   \n",
       "1              0.507812          2.382812              0.915507   \n",
       "2             -0.593750         -0.035156              0.491212   \n",
       "3              0.482422          1.267578              0.780328   \n",
       "4              1.554688          0.664062              0.660172   \n",
       "\n",
       "   RewardChosen_hermes  RewardRejected_hermes  RewardDiff_hermes  \\\n",
       "0            -1.021655              -1.308317           0.286662   \n",
       "1             1.413000              -0.526894           1.939894   \n",
       "2            -0.104880              -0.680847           0.575967   \n",
       "3             1.230862              -2.328661           3.559523   \n",
       "4             0.448723              -0.174440           0.623163   \n",
       "\n",
       "   PreferenceProb_hermes  PrefProbVariance  PrefProbAverage  RwDiffVariance  \\\n",
       "0               0.571179          0.027955         0.600188        0.559248   \n",
       "1               0.874340          0.011647         0.833760        0.577412   \n",
       "2               0.640139          0.011347         0.609653        0.199977   \n",
       "3               0.972335          0.009251         0.872932        1.412757   \n",
       "4               0.650938          0.000672         0.640831        0.012513   \n",
       "\n",
       "    GT  Error  CrossEntropy  \n",
       "0  1.0    0.0      0.110700  \n",
       "1  1.0    0.0      0.015570  \n",
       "2  1.0    0.0      0.104518  \n",
       "3  1.0    0.0      0.008830  \n",
       "4  1.0    0.0      0.085821  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69a3b8d2-186d-4749-a61e-d750e77067fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, eval_df, ood_df = split_dataset(joined_df, \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf1ba020-bd4e-4284-86a4-b3082ffc07eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Between Var(p) and Cross Entropy for Training: 0.23106520440146183\n",
      "Correlation Between Var(p) and Cross Entropy for OOD: 0.05782040031539965\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {train_df['PrefProbVariance'].corr(train_df['CrossEntropy'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for OOD: {ood_df['PrefProbVariance'].corr(ood_df['CrossEntropy'], method='spearman')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e7d51-fc79-49a3-aee4-94010ff00744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlation Between Var(p) and Cross Entropy for Training: {train_df['PrefProbVariance'].corr(train_df['BrierScore'], method='spearman')}\")\n",
    "\n",
    "print(f\"Correlation Between Var(p) and Cross Entropy for OOD: {ood_df['PrefProbVariance'].corr(ood_df['BrierScore'], method='spearman')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9d2336f-5ad4-4607-bbeb-ef5d33c95b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_labels = ['VI', 'AdaptEns', 'FinetuneEns', 'PretrainEns']\n",
    "bar_labels = ['train', 'test', 'eval', 'ood']\n",
    "# Set the width of the bars\n",
    "bar_width = 0.2\n",
    "\n",
    "# Generate some random data for the bars\n",
    "data = np.array([[0.6614296494944618, 0.4840945267028453 ,-0.09131798391076847 , 0.23106520440146183], [0.47892330221793167, 0.322236670433129, -0.014102754498991945, 0.0], [0.45251092137986665, 0.30988203179789975, 0.006720589136532155, 0.0], [0.36932937182651315, 0.22402185570435587, -0.11646054855009284, 0.05782040031539965]])\n",
    "\n",
    "# Create an array with the positions of each bar along the x-axis\n",
    "x_pos = np.arange(len(bar_labels))\n",
    "\n",
    "# For each set of bars: position, label, and data\n",
    "for i in range(len(x_labels)):\n",
    "    plt.bar(x_pos + i * bar_width, data[:, i], width=bar_width, label=x_labels[i])\n",
    "\n",
    "# Set the name of each x-axis label and the title of the plot\n",
    "plt.xticks(x_pos + bar_width, bar_labels)\n",
    "plt.title('Correlation')\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7901bc-f2f1-411c-a6a7-0f7fe077279c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbab54-fe30-4325-9fcc-3899ae30c7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
